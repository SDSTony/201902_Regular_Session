{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6주차\n",
    "## TNT 2019년 2학기 2부 발제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering and Principal Component Analysis\n",
    "이번 주차는 K-means와 PCA를 구현해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저번에 안한 사람만\n",
    "# socketio module 설치\n",
    "# !pip install python-socketio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "try:\n",
    "    pyplot.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "except ValueError:\n",
    "    pyplot.rcParams[\"animation.html\"] = \"html5\"\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from utils import Submit\n",
    "import answer\n",
    "\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = Submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# board 연결이 필요할 경우만 실행\n",
    "submission.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "먼저 2D data에 대해 K-means 알고리즘을 적용해보겠습니다.\n",
    "\n",
    "### 1.1 Implementing K-means\n",
    "K-means 알고리즘은 비슷한 데이터끼리 cluster를 만듭니다. 예를 들어 $\\{x^{(1)} , \\cdots, x^{(m)}\\}$ (where $x^{(i)} \\in \\mathbb{R}^n$) 과 같은 training set을 몇개의 밀도있는 cluster로 묶고 싶을 때 사용할 수 있습니다. K-means는 initial centroids 설정 이후, centroids와의 거리에 따른 cluster 부여, centroids update의 과정을 반복적으로 진행합니다.\n",
    "\n",
    "```python\n",
    "centroids = kMeansInitCentroids(X, K)\n",
    "for i in range(iterations):\n",
    "    # Cluster assignment step: 각각의 데이터에 가장 가까운 centroid를 할당.\n",
    "    idx = findClosestCentroids(X, centroids)\n",
    "    \n",
    "    # Move centroid step: centroids에 속한 데이터들의 평균으로 새로운 centroids 위치 설정\n",
    "    centroids = computeMeans(X, idx, K)\n",
    "```\n",
    "\n",
    "K-means 알고리즘은 항상 특정 결과로 수렴하지만, 그 결과는 최적점이 아닐 수도 있습니다. 이를 해결하기 위한 한 가지 방법은 서로 다른 초기값을 지정해 가장 낮은 cost를 갖는 모델을 선정하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Finding closest centroids\n",
    "K-means 알고리즘의 cluster assignment 단계에서 모든 데이터 $x^{(i)}$를 아래와 같은 조건에서 가장 가까운 centroid에 배정해야 합니다. \n",
    "\n",
    "$$c^{(i)} := j \\quad \\text{that minimizes} \\quad \\lvert\\rvert x^{(i)} - \\mu_j  \\lvert\\rvert^2, $$\n",
    "\n",
    "$c^{(i)}$는 $x^{(i)}$와 가장 가까운 centroid의 index입니다. 그리고 $\\mu_j$는 $j^{th}$ centroid의 값(또는 위치)입니다.\n",
    "data matrix `X`와 현재 centroids의 위치를 입력으로 받아 각 데이터에 배정된 centroid index를 출력하는 함수를 만들어보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestCentroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of size (m, n) where each row is a single example. \n",
    "        That is, we have m examples each of n dimensions.\n",
    "        \n",
    "    centroids : array_like\n",
    "        The k-means centroids of size (K, n). K is the number\n",
    "        of clusters, and n is the the data dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    idx : array_like\n",
    "        A vector of size (m, ) which holds the centroids assignment for each\n",
    "        example (row) in the dataset X.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every example, find its closest centroid, and store\n",
    "    the index inside `idx` at the appropriate location.\n",
    "    Concretely, idx[i] should contain the index of the centroid\n",
    "    closest to example i. Hence, it should be a value in the \n",
    "    range 0..K-1\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    You can use a for-loop over the examples to compute this.\n",
    "    \"\"\"\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    for i,j in enumerate(X):\n",
    "        distances = None\n",
    "        idx[i] = None  \n",
    "    \n",
    "    # =============================================================\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example dataset that we will be using위\n",
    "data = loadmat(os.path.join('Data', 'ex7data2.mat'))\n",
    "X = data['X']\n",
    "\n",
    "# Select an initial set of centroids\n",
    "K = 3   # 3 Centroids\n",
    "initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n",
    "\n",
    "# Find the closest centroids for the examples using the initial_centroids\n",
    "idx = findClosestCentroids(X, initial_centroids)\n",
    "\n",
    "print('Closest centroids for the first 3 examples:')\n",
    "print(idx[:3])\n",
    "print('(the closest centroids should be 0, 2, 1 respectively)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.submit(findClosestCentroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Computing centroid means\n",
    "각 데이터에 centroid가 배정되었다면, centroid의 위치를 데이터들의 평균으로 업데이트할 차례입니다.\n",
    "\n",
    "$$ \\mu_k := \\frac{1}{\\left| C_k\\right|} \\sum_{i \\in C_k} x^{(i)}$$\n",
    "\n",
    "$C_k$는 centroid $k$에 배정된 데이터들의 집합입니다. 예를 들어, 두 데이터 $x^{(3)}$와 $x^{(5)}$가 centroid $k = 2$에 배정되었다면, $\\mu_2 = \\frac{1}{2} \\left( x^{(3)} + x^{(5)} \\right)$로 업데이트 해야 합니다.centroid의 새로운 위치를 계산하는 `computeCentroids` 함수를 완성하세요. 당신이 초보자라면 loop 연산으로, 중급자라면 벡터 연산으로 계산해보세요. 설마 초급자겠어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCentroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the data points\n",
    "    assigned to each centroid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The datset where each row is a single data point. That is, it \n",
    "        is a matrix of size (m, n) where there are m datapoints each\n",
    "        having n dimensions. \n",
    "    \n",
    "    idx : array_like \n",
    "        A vector (size m) of centroid assignments (i.e. each entry in range [0 ... K-1])\n",
    "        for each example.\n",
    "    \n",
    "    K : int\n",
    "        Number of clusters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids : array_like\n",
    "        A matrix of size (K, n) where each row is the mean of the data \n",
    "        points assigned to it.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every centroid and compute mean of all points that\n",
    "    belong to it. Concretely, the row vector centroids[i, :]\n",
    "    should contain the mean of the data points assigned to\n",
    "    cluster i.\n",
    "\n",
    "    Note:\n",
    "    -----\n",
    "    You can use a for-loop over the centroids to compute this.\n",
    "    \"\"\"\n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    centroids = np.zeros((K, n))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    for i in range(K):\n",
    "        centroids[i] = None\n",
    "    \n",
    "    # =============================================================\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the code in `computeCentroids`, the following cell will run your code and output the centroids after the first step of K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means based on the closest centroids found in the previous part.\n",
    "centroids = computeCentroids(X, idx, K)\n",
    "\n",
    "print('Centroids computed after initial finding of closest centroids:')\n",
    "print(centroids)\n",
    "print('\\nThe centroids should be')\n",
    "print('   [ 2.428301 3.157924 ]')\n",
    "print('   [ 5.813503 2.633656 ]')\n",
    "print('   [ 7.119387 3.616684 ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.submit(computeCentroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 K-means on example dataset\n",
    "위의 두 함수 (`findClosestCentroids` and `computeCentroids`)를 성공적으로 완성했다면, 여러분들은 k-means 알고리즘에서 필요한 모든 것들을 만드셨습니다.다음으로 2D 데이터에 우리가 만든 알고리즘을 적용해보겠습니다.여러분이 만든 함수가 `runKmeans` 함수 안에서 동작할 것입니다. 아래의 그림과 같은 결과가 나오는 지 확인해보세요.\n",
    "\n",
    "![](Figures/kmeans_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load an example dataset\n",
    "data = loadmat(os.path.join('Data', 'ex7data2.mat'))\n",
    "\n",
    "# Settings for running K-Means\n",
    "K = 3\n",
    "max_iters = 10\n",
    "\n",
    "# For consistency, here we set centroids to specific values\n",
    "# but in practice you want to generate them automatically, such as by\n",
    "# settings them to be random examples (as can be seen in\n",
    "# kMeansInitCentroids).\n",
    "initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n",
    "\n",
    "\n",
    "# Run K-Means algorithm. The 'true' at the end tells our function to plot\n",
    "# the progress of K-Means\n",
    "centroids, idx, anim = answer.runkMeans(X, initial_centroids,\n",
    "                                        findClosestCentroids, \n",
    "                                        computeCentroids, \n",
    "                                        max_iters, True)\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Principal Component Analysis\n",
    "\n",
    "### 2.1 Example Dataset\n",
    "PCA를 통해 차원축소를 해보겠습니다. 먼저 간단한 2D 데이터에 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = loadmat(os.path.join('Data', 'ex7data1.mat'))\n",
    "X = data['X']\n",
    "\n",
    "# 데이터 시각화\n",
    "pyplot.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=1)\n",
    "pyplot.axis([0.5, 6.5, 2, 8])\n",
    "pyplot.gca().set_aspect('equal')\n",
    "pyplot.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementing PCA\n",
    "PCA는 아래와 같은 순서로 진행됩니다.\n",
    "1. data의 공분산 구하기\n",
    "2. SVD (`np.linalg.svd`)로 고유벡터 $U_1$, $U_2$, $\\dots$, $U_n$ 계산하기.\n",
    "\n",
    "먼저 아래와 같이 data의 공분산을 구하세요.\n",
    "$$ \\Sigma = \\frac{1}{m} X^T X$$\n",
    "$X$는 data matrix를, $m$은 데이터의 수를 의미합니다.\n",
    "\n",
    "공분산을 계산한 뒤 SVD를 적용하세요. `numpy`를 사용해 `U, S, V = np.linalg.svd(Sigma)`와 같이 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Run principal component analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset to be used for computing PCA. It has dimensions (m x n)\n",
    "        where m is the number of examples (observations) and n is \n",
    "        the number of features.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    U : array_like\n",
    "        The eigenvectors, representing the computed principal components\n",
    "        of X. U has dimensions (n x n) where each column is a single \n",
    "        principal component.\n",
    "    \n",
    "    S : array_like\n",
    "        A vector of size n, contaning the singular values for each\n",
    "        principal component. Note this is the diagonal of the matrix we \n",
    "        mentioned in class.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should first compute the covariance matrix. Then, you\n",
    "    should use the \"svd\" function to compute the eigenvectors\n",
    "    and eigenvalues of the covariance matrix. \n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    When computing the covariance matrix, remember to divide by m (the\n",
    "    number of examples).\n",
    "    \"\"\"\n",
    "    # Useful values\n",
    "    m, n = X.shape\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    Sigma = None\n",
    "    U, S, V = None\n",
    "\n",
    "    # ============================================================\n",
    "    return U, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA를 적용하기 전에 데이터를 normailze 해주는 것이 중요합니다. `featureNormalize` 함수로 normalize를 진행한 뒤 PCA를 적용해보세요.\n",
    "\n",
    "![](Figures/pca_components.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Before running PCA, it is important to first normalize X\n",
    "X_norm, mu, sigma = answer.featureNormalize(X)\n",
    "\n",
    "#  Run PCA\n",
    "U, S = pca(X_norm)\n",
    "\n",
    "#  Draw the eigenvectors centered at mean of data. These lines show the\n",
    "#  directions of maximum variations in the dataset.\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=0.25)\n",
    "\n",
    "for i in range(2):\n",
    "    ax.arrow(mu[0], mu[1], 1.5 * S[i]*U[0, i], 1.5 * S[i]*U[1, i],\n",
    "             head_width=0.25, head_length=0.2, fc='k', ec='k', lw=2, zorder=1000)\n",
    "\n",
    "ax.axis([0.5, 6.5, 2, 8])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "\n",
    "print('Top eigenvector: U[:, 0] = [{:.6f} {:.6f}]'.format(U[0, 0], U[1, 0]))\n",
    "print(' (you should expect to see [-0.707107 -0.707107])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.submit(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dimensionality Reduction with PCA\n",
    "주성분을 계산한 뒤, 여러분들의 데이터를 더 낮은 차원으로 투영할 수 있습니다.\n",
    "$x^{(i)} \\rightarrow z^{(i)}$ (e.g., projecting the data from 2D to 1D)\n",
    "여러분들이 학습 알고리즘에 축소된 데이터를 적용해보세요. 더 빠른 진행을 확인할 수 있습니다.\n",
    "\n",
    "### 2.3.1 Projecting the data onto the principal components\n",
    "`projectData` 함수를 완성하세요. `X`는 dataset, `U`는 주성분, `K`는 축소할 차원을 의미합니다. `Ureduce = U[:, :K]`의 방법으로 지정된 K개 선정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectData(X, U, K):\n",
    "    \"\"\"\n",
    "    Computes the reduced data representation when projecting only \n",
    "    on to the top K eigenvectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n). The dataset is assumed to be \n",
    "        normalized.\n",
    "    \n",
    "    U : array_like\n",
    "        The computed eigenvectors using PCA. This is a matrix of \n",
    "        shape (n x n). Each column in the matrix represents a single\n",
    "        eigenvector (or a single principal component).\n",
    "    \n",
    "    K : int\n",
    "        Number of dimensions to project onto. Must be smaller than n.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Z : array_like\n",
    "        The projects of the dataset onto the top K eigenvectors. \n",
    "        This will be a matrix of shape (m x k).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the projection of the data using only the top K \n",
    "    eigenvectors in U (first K columns). \n",
    "    For the i-th example X[i,:], the projection on to the k-th \n",
    "    eigenvector is given as follows:\n",
    "    \n",
    "        x = X[i, :]\n",
    "        projection_k = np.dot(x,  U[:, k])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    #Z = np.dot(X, U[:,:K])\n",
    "    Ureduce = None\n",
    "    Z = None\n",
    "    \n",
    "    # =============================================================\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project the data onto K = 1 dimension\n",
    "K = 1\n",
    "Z = projectData(X_norm, U, K)\n",
    "print('Projection of the first example: {:.6f}'.format(Z[0, 0]))\n",
    "print('(this value should be about    : 1.481274)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.submit(projectData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Reconstructing an approximation of the data\n",
    "차원축소한 데이터를 다시 복원할 수 있어야 합니다. `recoverData` 함수를 완성해 원래 차원으로 복원했을 때의 추정값 `Xrec`을 구하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverData(Z, U, K):\n",
    "    \"\"\"\n",
    "    Recovers an approximation of the original data when using the \n",
    "    projected data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : array_like\n",
    "        The reduced data after applying PCA. This is a matrix\n",
    "        of shape (m x K).\n",
    "    \n",
    "    U : array_like\n",
    "        The eigenvectors (principal components) computed by PCA.\n",
    "        This is a matrix of shape (n x n) where each column represents\n",
    "        a single eigenvector.\n",
    "    \n",
    "    K : int\n",
    "        The number of principal components retained\n",
    "        (should be less than n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_rec : array_like\n",
    "        The recovered data after transformation back to the original \n",
    "        dataset space. This is a matrix of shape (m x n), where m is \n",
    "        the number of examples and n is the dimensions (number of\n",
    "        features) of original datatset.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the approximation of the data by projecting back\n",
    "    onto the original space using the top K eigenvectors in U.\n",
    "    For the i-th example Z[i,:], the (approximate)\n",
    "    recovered data for dimension j is given as follows:\n",
    "\n",
    "        v = Z[i, :]\n",
    "        recovered_j = np.dot(v, U[j, :K])\n",
    "\n",
    "    Notice that U[j, :K] is a vector of size K.\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    X_rec = np.zeros((Z.shape[0], U.shape[0]))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    X_rec = None\n",
    "\n",
    "    # =============================================================\n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the code in `recoverData`, the following cell will recover an approximation of the first example and you should see a value of about `[-1.047 -1.047]`. The code will then plot the data in this reduced dimension space. This will show you what the data looks like when using only the corresponding eigenvectors to reconstruct it. An example of what you should get for PCA projection is shown in this figure: \n",
    "\n",
    "![](Figures/pca_reconstruction.png)\n",
    "\n",
    "In the figure above, the original data points are indicated with the blue circles, while the projected data points are indicated with the red circles. The projection effectively only retains the information in the direction given by $U_1$. The dotted lines show the distance from the data points in original space to the projected space. Those dotted lines represent the error measure due to PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rec  = recoverData(Z, U, K)\n",
    "print('Approximation of the first example: [{:.6f} {:.6f}]'.format(X_rec[0, 0], X_rec[0, 1]))\n",
    "print('       (this value should be about  [-1.047419 -1.047419])')\n",
    "\n",
    "#  Plot the normalized dataset (returned from featureNormalize)\n",
    "fig, ax = pyplot.subplots(figsize=(5, 5))\n",
    "ax.plot(X_norm[:, 0], X_norm[:, 1], 'bo', ms=8, mec='b', mew=0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(False)\n",
    "pyplot.axis([-3, 2.75, -3, 2.75])\n",
    "\n",
    "# Draw lines connecting the projected points to the original points\n",
    "ax.plot(X_rec[:, 0], X_rec[:, 1], 'ro', mec='r', mew=2, mfc='none')\n",
    "for xnorm, xrec in zip(X_norm, X_rec):\n",
    "    ax.plot([xnorm[0], xrec[0]], [xnorm[1], xrec[1]], '--k', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.submit(recoverData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

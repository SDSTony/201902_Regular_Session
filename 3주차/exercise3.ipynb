{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise3\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 임포트합니다.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize # optimization\n",
    "\n",
    "#import utils\n",
    "#grader = utils.Grader()\n",
    "\n",
    "np.random.seed(42) # reproduction을 위한 random seed 고정\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "logistic regression을 활용하여 분류 모델을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# X : column 0, 1\n",
    "# y : column 2\n",
    "data = np.loadtxt(os.path.join('Data', 'ex2data1.txt'), delimiter=',')\n",
    "X, y = data[:, 0:2], data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 확인\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualize the data\n",
    "본격적으로 분류를 하기 전에 데이터를 시각화 해보겠습니다. plotData 함수로 2차원 플롯을 그려보겠습니다. X1, X2를 각각의 축으로 삼고 라벨에 따라 다른 마커로 데이터를 표시해주세요. 아래의 코드를 참고하여 plotData 함수를 작성해주세요.\n",
    "\n",
    "```python\n",
    "# Find Indices of Positive and Negative Examples\n",
    "pos = y == 1\n",
    "neg = y == 0\n",
    "\n",
    "# Plot Examples\n",
    "plt.plot(X[pos, 0], X[pos, 1], 'k*', lw=2, ms=10)\n",
    "plt.plot(X[neg, 0], X[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    \n",
    "    fig = pyplot.figure()\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend(['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Sigmoid function\n",
    "\n",
    "> sigmoid function의 정의 : $ g(z) = \\frac{1}{1+e^{-z}} $.\n",
    "\n",
    "첫번째 과제는 sigmoid function을 구현하는 것입니다. \n",
    "\n",
    "X가 커질수록 1에 가까운 값이 나오고, 작아질수록 0에 가까운 값이 나오고, 0일 경우 0.5의 값이 나와야합니다. \n",
    "\n",
    "벡터와 행렬 입력에서 작동해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정한 array y에 대해 exponential 을 적용해보세요\n",
    "np.exp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # 입력 z를 array로 변환\n",
    "    z = np.array(z)\n",
    "    \n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # 변수 g에 할당해주세요.\n",
    "    \n",
    "    g = None\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# 0을 넣을 경우 0.5를 출력해야 합니다.\n",
    "z = 0\n",
    "g = sigmoid(z)\n",
    "\n",
    "print('g(', z, ') = ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[0] = sigmoid\n",
    "grader.grade(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Cost function and Gradient\n",
    "logistic regression의 cost function과 gradient를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X에 상수항 추가 (한번만 해주세요)\n",
    "m, n = X.shape\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> logistic regression의 hypothesis : $ h_\\theta(x) = g(\\theta^T x)$\n",
    "\n",
    "> cost function : $ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right] $\n",
    "\n",
    "> gradient of $\\theta_j$ : $ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $\n",
    "\n",
    "gradient의 정의가 linear regression과 동일해보이지만, hypothesis가 다르기 때문에 차이가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta가 [A, B, C] 형태로 주어졌을때 각 data point에 대해 hypothesis 를 구해보세요\n",
    "# A*X0 + B*X1 + C*X2 에 앞서 정의한 sigmoid 함수가 적용된 후 각 data point에 대한 hypothesis 값이 나와야합니다.\n",
    "# 따라서 (m,)의 shape을 가진 array로 출력이 되어야 합니다.\n",
    "\n",
    "theta = [0.01, 0.01, 0.01] # 예제 theta\n",
    "hypothesis = None\n",
    "\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 구한 hypothesis 값을 이용해 Cost를 출력해 보세요\n",
    "# 각 data point들에 대해 각각 계산이 되어 (m,) 형태의 array로 출력이 되어야 합니다.\n",
    "J_array = None\n",
    "print(J_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 앞에서 구한 각 data point들의 값을 다 더하고 총 데이터의 개수로 나누어 주어 이를 J에 할당하겠습니다.\n",
    "# 총 데이터의 개수는 m 입니다.\n",
    "m = J_array.size\n",
    "J = np.sum(J_array) / m\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta에 대한 gradient를 구해 보겠습니다\n",
    "# 에시로 theta[0]에 대해 gradient를 구해 보겠습니다.\n",
    "# 첫번째로 앞서 구한 hypothesis 값을 y로 뺀 다음 X의 0번째 열을 곱해주겠습니다. 즉 (hypothesis - y)*X[0] 가 되어야 합니다.\n",
    "\n",
    "grad_theta0_array = None\n",
    "print(grad_theta0_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 각 data point의 gradient 값의 평균을 구합니다.\n",
    "grad_theta0 = None\n",
    "print(grad_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같은 과정을 일반화해서 함수로 나타내 주세요! Main 문제 입니다.\n",
    "# gradient를 구할때 for 문을 이용해 여러개의 theta에 대해 반복적으로 진행하게 나타낼 수 있습니다.\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # train data의 수\n",
    "    grad = np.ones_like(theta) # grad 틀\n",
    "\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    J = None\n",
    "    grad = None\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(n+1)\n",
    "\n",
    "cost, grad = costFunction(initial_theta, X, y)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.693\\n')\n",
    "\n",
    "print('Gradient at initial theta (zeros):')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}]'.format(*grad))\n",
    "print('Expected gradients (approx):\\n\\t[-0.1000, -12.0092, -11.2628]\\n')\n",
    "\n",
    "# Compute and display cost and gradient with non-zero theta\n",
    "test_theta = np.array([-24, 0.2, 0.2])\n",
    "cost, grad = costFunction(test_theta, X, y)\n",
    "\n",
    "print('Cost at test theta: {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.218\\n')\n",
    "\n",
    "print('Gradient at test theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*grad))\n",
    "print('Expected gradients (approx):\\n\\t[0.043, 2.566, 2.647]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer(1) = costFunction\n",
    "grader.grade(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer(2) = costFunction\n",
    "grader.grade(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 `scipy.optimize`로 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 gradient descent alorithm을 직접 구현해 최적화를 했습니다. 오늘은 `scipy.optimize` 함수를 통해 최적화를 실시하겠습니다.\n",
    "`optimize.minimize`를 통해 cost function $J(\\theta)$ 를 최소화하는 parameter $\\theta$ 찾아보겠습니다.\n",
    "\n",
    "`optimize.minimize`는 다음과 같은 파라미터를 필요로 합니다.\n",
    "* `costFunction` : 최적화시킬 비용함수\n",
    "* `initial_theta` : 파라미터의 초기값\n",
    "* `(X, y)` : 데이터\n",
    "* `jac` : gradient vector 출력여부 (True)\n",
    "* `method` : 최적화 알고리즘\n",
    "* `options` : 기타 튜닝 옵션\n",
    "\n",
    "`optimize.minize`를 쓸 때 loop를 쓰거나, learning rate를 지정할 필요가 없습니다. 최적화가 진행된 후 cost와 $\\theta$를 출력하게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 400}\n",
    "\n",
    "# The function returns an object `OptimizeResult`\n",
    "result = optimize.minimize(costFunction,\n",
    "                        initial_theta,\n",
    "                        (X, y),\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# the fun property of `OptimizeResult` object returns\n",
    "cost = result.fun # 최종 cost\n",
    "theta = result.x # 최종 theta\n",
    "\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by optimize.minimize: {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.203\\n');\n",
    "\n",
    "print('theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*theta))\n",
    "print('Expected theta (approx):\\n\\t[-25.161, 0.206, 0.201]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Evaluating logistic regression\n",
    "최적화된 모델의 성능을 확인해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저 앞에서 optimize로 구한 최적의 theta값을 이용해 hypothesis 를 구해보겠습니다.\n",
    "# sigmoid function을 이용해 (m,) shape의 array가 나오면 됩니다.\n",
    "hypothesis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 hypothesis를 0과 1로 분류해야합니다.\n",
    "# 각 data point의 값이 0.5보다 작으면 0, 0.5보다 크면 1입니다.\n",
    "# list comprehension or map 함수를 사용해보세요\n",
    "predict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "stic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    p = None\n",
    "    # ============================================================\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 예제\n",
    "prob = sigmoid(np.dot([1, 45, 85], theta))\n",
    "print('For a student with scores 45 and 85,'\n",
    "      'we predict an admission probability of {:.3f}'.format(prob))\n",
    "print('Expected value: 0.775 +/- 0.002\\n')\n",
    "\n",
    "# Compute accuracy on our training set\n",
    "p = predict(theta, X)\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == y) * 100))\n",
    "print('Expected accuracy (approx): 89.00 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[4] = predict\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularized Logistic Regression\n",
    "regularized logistic regression을 구현해보도록 하겠습니다.먼저 데이터를 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# X : column 0, 1\n",
    "# y : column 2\n",
    "data = np.loadtxt(os.path.join('Data', 'ex2data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.051267,  0.69956 ,  1.      ],\n",
       "       [-0.092742,  0.68494 ,  1.      ],\n",
       "       [-0.21371 ,  0.69225 ,  1.      ],\n",
       "       [-0.375   ,  0.50219 ,  1.      ],\n",
       "       [-0.51325 ,  0.46564 ,  1.      ],\n",
       "       [-0.52477 ,  0.2098  ,  1.      ],\n",
       "       [-0.39804 ,  0.034357,  1.      ],\n",
       "       [-0.30588 , -0.19225 ,  1.      ],\n",
       "       [ 0.016705, -0.40424 ,  1.      ],\n",
       "       [ 0.13191 , -0.51389 ,  1.      ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.051267,  0.69956 ],\n",
       "        [-0.092742,  0.68494 ],\n",
       "        [-0.21371 ,  0.69225 ],\n",
       "        [-0.375   ,  0.50219 ],\n",
       "        [-0.51325 ,  0.46564 ],\n",
       "        [-0.52477 ,  0.2098  ],\n",
       "        [-0.39804 ,  0.034357],\n",
       "        [-0.30588 , -0.19225 ],\n",
       "        [ 0.016705, -0.40424 ],\n",
       "        [ 0.13191 , -0.51389 ]]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  5.12670000e-02,  6.99560000e-01, ...,\n",
       "         6.29470940e-04,  8.58939846e-03,  1.17205992e-01],\n",
       "       [ 1.00000000e+00, -9.27420000e-02,  6.84940000e-01, ...,\n",
       "         1.89305413e-03, -1.39810280e-02,  1.03255971e-01],\n",
       "       [ 1.00000000e+00, -2.13710000e-01,  6.92250000e-01, ...,\n",
       "         1.04882142e-02, -3.39734512e-02,  1.10046893e-01],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -4.84450000e-01,  9.99270000e-01, ...,\n",
       "         2.34007252e-01, -4.82684337e-01,  9.95627986e-01],\n",
       "       [ 1.00000000e+00, -6.33640000e-03,  9.99270000e-01, ...,\n",
       "         4.00328554e-05, -6.31330588e-03,  9.95627986e-01],\n",
       "       [ 1.00000000e+00,  6.32650000e-01, -3.06120000e-02, ...,\n",
       "         3.51474517e-07, -1.70067777e-08,  8.22905998e-10]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n",
    "\n",
    "### 2.2 Feature mapping\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. In the function `mapFeature` defined in the file `utils.py`, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.\n",
    "\n",
    "$$ \\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T $$\n",
    "\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

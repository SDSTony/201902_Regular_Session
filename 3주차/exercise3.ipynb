{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise3\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 임포트합니다.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize # optimization\n",
    "\n",
    "import utils\n",
    "grader = utils.Grader()\n",
    "\n",
    "np.random.seed(42) # reproduction을 위한 random seed 고정\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "logistic regression을 활용하여 분류 모델을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# X : column 0, 1\n",
    "# y : column 2\n",
    "data = np.loadtxt(os.path.join('Data', 'ex2data1.txt'), delimiter=',')\n",
    "X, y = data[:, 0:2], data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 확인\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualize the data\n",
    "본격적으로 분류를 하기 전에 데이터를 시각화 해보겠습니다. plotData 함수로 2차원 플롯을 그려보겠습니다. X1, X2를 각각의 축으로 삼고 라벨에 따라 다른 마커로 데이터를 표시해주세요. 아래의 코드를 참고하여 plotData 함수를 작성해주세요.\n",
    "\n",
    "```python\n",
    "# Find Indices of Positive and Negative Examples\n",
    "pos = y == 1\n",
    "neg = y == 0\n",
    "\n",
    "# Plot Examples\n",
    "plt.plot(X[pos, 0], X[pos, 1], 'k*', lw=2, ms=10)\n",
    "plt.plot(X[neg, 0], X[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    \n",
    "    fig = pyplot.figure()\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.legend(['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Sigmoid function\n",
    "\n",
    "> sigmoid function의 정의 : $ g(z) = \\frac{1}{1+e^{-z}} $.\n",
    "\n",
    "첫번째 과제는 sigmoid function을 구현하는 것입니다. \n",
    "\n",
    "X가 커질수록 1에 가까운 값이 나오고, 작아질수록 0에 가까운 값이 나오고, 0일 경우 0.5의 값이 나와야합니다. \n",
    "\n",
    "벡터와 행렬 입력에서 작동해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정한 array y에 대해 exponential 을 적용해보세요\n",
    "np.exp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # 입력 z를 array로 변환\n",
    "    z = np.array(z)\n",
    "    \n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # 변수 g에 할당해주세요.\n",
    "    \n",
    "    g = None\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# 0을 넣을 경우 0.5를 출력해야 합니다.\n",
    "z = 0\n",
    "g = sigmoid(z)\n",
    "\n",
    "print('g(', z, ') = ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[0] = sigmoid\n",
    "grader.grade(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Cost function and Gradient\n",
    "logistic regression의 cost function과 gradient를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X에 상수항 추가 (한번만 해주세요)\n",
    "m, n = X.shape\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> logistic regression의 hypothesis : $ h_\\theta(x) = g(\\theta^T x)$\n",
    "\n",
    "> cost function : $ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right] $\n",
    "\n",
    "> gradient of $\\theta_j$ : $ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $\n",
    "\n",
    "gradient의 정의가 linear regression과 동일해보이지만, hypothesis가 다르기 때문에 차이가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta가 [A, B, C] 형태로 주어졌을때 각 data point에 대해 hypothesis 를 구해보세요\n",
    "# A*X0 + B*X1 + C*X2 에 앞서 정의한 sigmoid 함수가 적용된 후 각 data point에 대한 hypothesis 값이 나와야합니다.\n",
    "# 따라서 (m,)의 shape을 가진 array로 출력이 되어야 합니다.\n",
    "\n",
    "theta = [0.01, 0.01, 0.01] # 예제 theta\n",
    "hypothesis = None\n",
    "\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 구한 hypothesis 값을 이용해 Cost를 출력해 보세요\n",
    "# 각 data point들에 대해 각각 계산이 되어 (m,) 형태의 array로 출력이 되어야 합니다.\n",
    "J_array = None\n",
    "print(J_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 앞에서 구한 각 data point들의 값을 다 더하고 총 데이터의 개수로 나누어 주어 이를 J에 할당하겠습니다.\n",
    "# 총 데이터의 개수는 m 입니다.\n",
    "m = J_array.size\n",
    "J = None\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta에 대한 gradient를 구해 보겠습니다\n",
    "# 에시로 theta[0]에 대해 gradient를 구해 보겠습니다.\n",
    "# 첫번째로 앞서 구한 hypothesis 값을 y로 뺀 다음 X의 0번째 열을 곱해주겠습니다. 즉 (hypothesis - y)*X[0] 가 되어야 합니다.\n",
    "# (m,) shape의 array가 반환되면 됩니다.\n",
    "\n",
    "grad_theta0_array = None\n",
    "print(grad_theta0_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 각 data point의 gradient 값의 평균을 구합니다.\n",
    "# 한개의 숫자가 나오면 됩니다.\n",
    "grad_theta0 = None\n",
    "print(grad_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같은 과정을 일반화해서 함수로 나타내 주세요! Main 문제 입니다.\n",
    "# gradient를 구할때 for 문을 이용해 여러개의 theta에 대해 반복적으로 진행하게 나타낼 수 있습니다.\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # train data의 수\n",
    "    grad = np.ones_like(theta) # grad 틀\n",
    "\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    \n",
    "    J = None\n",
    "    grad = None\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(n+1)\n",
    "\n",
    "cost, grad = costFunction(initial_theta, X, y)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.693\\n')\n",
    "\n",
    "print('Gradient at initial theta (zeros):')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}]'.format(*grad))\n",
    "print('Expected gradients (approx):\\n\\t[-0.1000, -12.0092, -11.2628]\\n')\n",
    "\n",
    "# Compute and display cost and gradient with non-zero theta\n",
    "test_theta = np.array([-24, 0.2, 0.2])\n",
    "cost, grad = costFunction(test_theta, X, y)\n",
    "\n",
    "print('Cost at test theta: {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.218\\n')\n",
    "\n",
    "print('Gradient at test theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*grad))\n",
    "print('Expected gradients (approx):\\n\\t[0.043, 2.566, 2.647]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[1] = costFunction\n",
    "grader.grade(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 `scipy.optimize`로 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 gradient descent alorithm을 직접 구현해 최적화를 했습니다. 오늘은 `scipy.optimize` 함수를 통해 최적화를 실시하겠습니다.\n",
    "`optimize.minimize`를 통해 cost function $J(\\theta)$ 를 최소화하는 parameter $\\theta$ 찾아보겠습니다.\n",
    "\n",
    "`optimize.minimize`는 다음과 같은 파라미터를 필요로 합니다.\n",
    "* `costFunction` : 최적화시킬 비용함수\n",
    "* `initial_theta` : 파라미터의 초기값\n",
    "* `(X, y)` : 데이터\n",
    "* `jac` : gradient vector 출력여부 (True)\n",
    "* `method` : 최적화 알고리즘\n",
    "* `options` : 기타 튜닝 옵션\n",
    "\n",
    "`optimize.minize`를 쓸 때 loop를 쓰거나, learning rate를 지정할 필요가 없습니다. 최적화가 진행된 후 cost와 $\\theta$를 출력하게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 400}\n",
    "\n",
    "# The function returns an object `OptimizeResult`\n",
    "result = optimize.minimize(costFunction,\n",
    "                        initial_theta,\n",
    "                        (X, y),\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# the fun property of `OptimizeResult` object returns\n",
    "cost = result.fun # 최종 cost\n",
    "theta = result.x # 최종 theta\n",
    "\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by optimize.minimize: {:.3f}'.format(cost))\n",
    "print('Expected cost (approx): 0.203\\n');\n",
    "\n",
    "print('theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*theta))\n",
    "print('Expected theta (approx):\\n\\t[-25.161, 0.206, 0.201]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Evaluating logistic regression\n",
    "최적화된 모델의 성능을 확인해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저 앞에서 optimize로 구한 최적의 theta값을 이용해 hypothesis 를 구해보겠습니다.\n",
    "# sigmoid function을 이용해 (m,) shape의 array가 나오면 됩니다.\n",
    "hypothesis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 hypothesis를 0과 1로 분류해야합니다.\n",
    "# 각 data point의 값이 0.5보다 작으면 0, 0.5보다 크면 1입니다.\n",
    "# list comprehension or map 함수를 사용해보세요\n",
    "predict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    p = None\n",
    "    # ============================================================\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 예제\n",
    "prob = sigmoid(np.dot([1, 45, 85], theta))\n",
    "print('For a student with scores 45 and 85,'\n",
    "      'we predict an admission probability of {:.3f}'.format(prob))\n",
    "print('Expected value: 0.775 +/- 0.002\\n')\n",
    "\n",
    "# Compute accuracy on our training set\n",
    "p = predict(theta, X)\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == y) * 100))\n",
    "print('Expected accuracy (approx): 89.00 %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[2] = predict\n",
    "grader.grade(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularized Logistic Regression\n",
    "regularized logistic regression을 구현해보도록 하겠습니다.먼저 데이터를 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# X : column 0, 1\n",
    "# y : column 2\n",
    "data = np.loadtxt(os.path.join('Data', 'ex2data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature mapping\n",
    "더 나은 성능의 모델을 만들기 위해 새로운 feature를 만들 것입니다. `sklearn.preprocessing`의 `PolynomialFeatures`을 통해 polynomial combinations을 생성할 수 있습니다. 6 degree의 feature를 만들어보겠습니다.\n",
    "\n",
    "$$ \\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(6)\n",
    "X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 polynomial feature를 만들게되면 총 28개의 feature가 만들어집니다. 하지만 overfitting의 위험이 크기 때문에 regularization을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cost function and gradient\n",
    "> regularzied cost function : $ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $\n",
    "\n",
    "> gradient of $\\theta_0$ : $ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $\n",
    "\n",
    "> gradient of $\\theta_j$ : $ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $\n",
    "\n",
    "$\\theta_0$ 에 대해서는 regularization을 진행하지 않기 때문에 위와 같이 분리해서 gradient를 구해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta가 [A, B, C, D, E, F]로 주어졌을때 B부터 F까지의 수를 각각 제곱하여 더한 값을 구해봅시다\n",
    "# 하나의 숫자의 형태로 반환하면 됩니다\n",
    "theta = np.random.rand(6)\n",
    "tt = None\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    # Compute cost and gradient for logistic regression with regularization.\n",
    "    # Initialize some useful values\n",
    "    # lambda_ : regularization parameter\n",
    "    \n",
    "    m = y.size  # number of training examples\n",
    "    grad = np.ones_like(theta)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    \n",
    "    J = None\n",
    "    grad = None\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 초기화\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "\n",
    "# reagularization parameter를 1로 지정\n",
    "# 'lambda'는 python 내장 변수이기 때문에 변수명으로 지정하면 안됩니다.\n",
    "lambda_ = 1\n",
    "\n",
    "cost, grad = costFunctionReg(initial_theta, X, y, lambda_)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))\n",
    "print('Expected cost (approx)       : 0.693\\n')\n",
    "\n",
    "print('Gradient at initial theta (zeros) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n",
    "print('Expected gradients (approx) - first five values only:')\n",
    "print('\\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\\n')\n",
    "\n",
    "\n",
    "# Compute and display cost and gradient\n",
    "# with all-ones theta and lambda = 10\n",
    "test_theta = np.ones(X.shape[1])\n",
    "cost, grad = costFunctionReg(test_theta, X, y, 10)\n",
    "\n",
    "print('------------------------------\\n')\n",
    "print('Cost at test theta    : {:.2f}'.format(cost))\n",
    "print('Expected cost (approx): 3.16\\n')\n",
    "\n",
    "print('Gradient at initial theta (zeros) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n",
    "print('Expected gradients (approx) - first five values only:')\n",
    "print('\\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.answer[3] = costFunctionReg\n",
    "grader.grade(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

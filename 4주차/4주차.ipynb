{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxjYAOngnrCR"
   },
   "source": [
    "# 4주차\n",
    "## Regularized Linear Regression and Bias vs Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkmF77rqnrCS"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# scipy내에 있는 최적화기능이 있는 모듈\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# MATLAB의 mat 형식의 파일을 읽기 위해 사용\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# 제출을 하기 위한 라이브러리\n",
    "sys.path.append('../src/')\n",
    "from utils import submit\n",
    "from utils import help_me\n",
    "from answer import trainLinearReg, featureNormalize, plotFit\n",
    "# matplotlib가 노트북에 플롯을 포함하도록 지시\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQt9RAnpnrCW"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1 Regularized Linear Regression\n",
    "\n",
    "이번 문제에서는, 저수지의 물의 양이 달라짐에 따라 댐에서 나오는 물의 양이 어떻게 변하는지 regularized linear regression을 통해 예측해 봅시다. bias와 variance의 변화를 보면서 알맞은 lambda값을 찾아 볼 것입니다. \n",
    "\n",
    "### 1.1 Visualizing the dataset\n",
    "\n",
    "우선 데이터가 어떻게 구성되어 있는지 봅시다. \n",
    "\n",
    "- theta를 학습시키기 위한 training set: `X`, `y`\n",
    "- lambda값을 조정시키기 위한 cross validation set: `Xval`, `yval`\n",
    "- 모델의 성능을 평가하기 위한 test set: `Xtest`, `ytest`\n",
    "\n",
    "우선 다음 셀을 실행시켜서 training set을 시각화 해보세요. 그리고 나서 linear regression을 사용해서 training set에 직선을 fitting 시킬 것입니다. 그 후에는, polynomial regression을 활용해서 training set에 더 알맞은 선을 fitting 시킬 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW-IkutjnrCW"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기, dict 형태로 저장됩니다. \n",
    "data = loadmat(os.path.join('Data', 'ex5data1.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVumLzIuVN95",
    "outputId": "0fd805b8-8fb8-4635-b216-4427401c4a76"
   },
   "outputs": [],
   "source": [
    "# 데이터 나누기\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "Xtest, ytest = data['Xtest'], data['ytest'][:, 0]\n",
    "Xval, yval = data['Xval'], data['yval'][:, 0]\n",
    "\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)\n",
    "print('Xval:', Xval.shape)\n",
    "print('yval:', yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCOoVfAeVN-A"
   },
   "outputs": [],
   "source": [
    "# training set의 sample 개수\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOCbWUsmVN-F",
    "outputId": "448afc44-c564-4922-e8f4-d682d87872c9"
   },
   "outputs": [],
   "source": [
    "# training set 시각화 \n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCfFFxPLnrCY"
   },
   "source": [
    "### 1.2 Regularized linear regression cost function\n",
    "Regularized linear regression의 cost function은 다음과 같습니다.:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\left( \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right) + \\frac{\\lambda}{2m} \\left( \\sum_{j=1}^n \\theta_j^2 \\right)$$\n",
    "\n",
    "$\\lambda$가 regularization의 정도를 결정합니다. 그렇게 함으로써 overfitting을 방지합니다. lambda가 증가할 수록 cost function은 패널티를 더 많이 받게 됩니다. 참고로 $\\theta_0$은 상수항에 대한 파라미터이기 때문에 패널티를 부여하지 않습니다. \n",
    "\n",
    "다음 셀에 있는 `linearRegCostFunction`을 완성해보세요. 목표는 regularized linear regression의 cost를 계산하는 것입니다. 가능한 for문을 사용하지 말고 행렬로 한번에 계산해 보세요. \n",
    "<a id=\"linearRegCostFunction\"></a>\n",
    "\n",
    "간단한 예제 theta를 통해 연습한 후 진짜 함수를 정의해보겠습니다  \n",
    "차례대로 따라간 후 연습한 문항을 통해 함수를 정의해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ao5STaVmbZKd"
   },
   "outputs": [],
   "source": [
    "# 연습을 위해 X데이터에 상수항을 추가하겠습니다. 프린트하시고 형태 확인해보세욥\n",
    "X_exer = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "print(X_exer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GA9SQpEKXz81"
   },
   "outputs": [],
   "source": [
    "# theta와 데이터 X_exer가 주어졌을 때 hypothesis 값을 구해보겠습니다\n",
    "theta = np.random.rand(X_exer.shape[1])\n",
    "hypothesis = None\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAKU_u_4ZQXf"
   },
   "outputs": [],
   "source": [
    "# 앞서 구한 hypothesis 값을 이용해 J의 앞부분 (regularization 부분을 제외한) 을 먼저 구해보겠습니다\n",
    "J_A = None\n",
    "print(J_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qodUuD0vZ01Z"
   },
   "outputs": [],
   "source": [
    "# 이제 J의 뒷부분을 구해보겠습니다. theta[0] 값은 cost function에 포함되지 않는걸 주의하세요!\n",
    "# theta[0] 값을 제외하면 theta 값이 1개 밖에 없지만, 여러개가 있다고 생각하고 일반화해서 계산해주세요\n",
    "lambda_ = 1\n",
    "J_B = None\n",
    "print(J_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZG82kJ3dWcm"
   },
   "source": [
    "지금까지 연습한 부분을 바탕으로 J 값을 구해주시면 됩니다!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qn_52n5raP6"
   },
   "source": [
    "### 1.3 Regularized linear regression gradient\n",
    "\n",
    "gradient는 다음과 같이 계산됩니다.: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "다음 셀에 있는 `lrgradient` 를 계산해보세요. 목표는 regularized linear regression의 grad를 구하는 것입니다.\n",
    "\n",
    "마찬가지로 간단한 연습 후에 진짜 함수를 정의해 보겠습니다.  \n",
    "theta[1] ~ 의 gradient를 구할 때 for문을 사용하지 않고 행렬 계산을 통해 진행해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1k29yOqrk9g"
   },
   "outputs": [],
   "source": [
    "# 앞서 정의한 hypothesis, theta, X_exer 를 통해 theta[0] 의 gradient를 구해보겠습니다\n",
    "# theta[0] 의 gradient는 theta값을 포함하지 않는것을 주의하세요\n",
    "grad_exer = np.zeros(theta.size)\n",
    "grad_exer[0] = None\n",
    "grad_exer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdAJB_sxrrZr"
   },
   "outputs": [],
   "source": [
    "# 이제 theta[0]를 제외한 나머지 theta의 gradient를 구해야합니다\n",
    "# 연습문항에는 theta[0]를 제외하면 theta 값이 1개 밖에 없지만, 여러개가 있다고 생각하고 일반화해서 계산해주세요\n",
    "# 먼저 gradient의 앞부분(lambda_가 들어가는 부분을 제외한) 을 먼저 계산해보겠습니다.\n",
    "grad_exer_A = None\n",
    "grad_exer_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DY_Kc2TwrsUZ"
   },
   "outputs": [],
   "source": [
    "# 이제 뒷부분을 계산해보겠습니다\n",
    "lambda_ = 1\n",
    "grad_exer_B = None\n",
    "grad_exer_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYGxwGU9rzKl"
   },
   "source": [
    "이제 연습한 부분을 바탕으로 함수를 만들어 보겠습니다  \n",
    "grad[1:] 는 앞서 연습한 부분에서 grad_exer_A 와 grad_exer_B를 더해주기만 하면 됩니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vjkPn0enrCZ"
   },
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    regularized linear regression의 cost와 gradient을 계산하는 함수 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        (m x n + 1) 크기의 행렬. m은 sample의 개수이고, n은 \n",
    "        상수항을 더하기 이전의 feature의 개수 \n",
    "        \n",
    "    y : array_like\n",
    "        각 sample들의 label값\n",
    "        크기는 (m, )\n",
    "        \n",
    "    theta : array_like\n",
    "        모델의 파라미터.\n",
    "        크기는 (n+1, )\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        regularization 파라미터 \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        cost 값\n",
    "    \n",
    "    grad : array_like\n",
    "        theta별 gradient 값\n",
    "        크기는 (n+1, )\n",
    "        \n",
    "    \"\"\"\n",
    "    # training 샘플 개수 \n",
    "    m = y.size \n",
    "\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    hypothesis = None\n",
    "    J = None\n",
    "    grad[0] = None \n",
    "    grad[1:] = None\n",
    "    # ============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1HBhxGZnrCb"
   },
   "source": [
    "`theta` 를 `[1, 1]`로 초기화하고 다음 셀을 실행하면 303.993192가 나와야 할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkjFa0ZknrCc",
    "outputId": "59847e77-8aff-444a-a5db-04b829fe7ade"
   },
   "outputs": [],
   "source": [
    "theta = np.array([1, 1])\n",
    "J, _ = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)\n",
    "\n",
    "print('Cost at theta = [1, 1] : {}'.format(round(J, 6)))\n",
    "print('(This value should be about 303.993192)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2EPUOOgnrCg"
   },
   "source": [
    "다음셀의 값은 [-15.303016, 598.250744]`가 나와야 할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BF5iL0x9nrCh",
    "outputId": "4f947240-7ea8-418b-a01f-deee593c1e08"
   },
   "outputs": [],
   "source": [
    "theta = np.array([1, 1])\n",
    "J, grad = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)\n",
    "\n",
    "print('Gradient at theta = [1, 1]:  [{:.6f}, {:.6f}] '.format(*grad))\n",
    "print(' (this value should be about [-15.303016, 598.250744])\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7e8QXaFnrCe"
   },
   "source": [
    "답이 맞는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9FadZejnrCe"
   },
   "outputs": [],
   "source": [
    "submit(linearRegCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_HKCLZ9enrCl"
   },
   "source": [
    "### Fitting linear regression\n",
    "\n",
    "다음 셀을 실행시키면, 최적의 theta 값을 찾게 될 것입니다. `trainLinearReg`을 사용하면 저번주에 사용했던 `scipy`의 optimize 모듈을 사용해서 최적의 theta값을 찾게 됩니다. \n",
    "\n",
    "여기에서는 두개의 feature (상수항이랑 원래 feature)에 대해서만 학습을 진행하기 때문에, lambda를 0으로 두겠습니다. 왜냐하면 feature의 수가 너무 적어서 regularization을 해봤자 별 도움이 안됩니다. \n",
    "\n",
    "아래 코드를 실행하면 다음과 같은 그림이 나와야 할 것입니다. \n",
    "\n",
    "![](Figures/linear_fit.png)\n",
    "\n",
    "보시면은 training set에는 비선형적인 패턴이 있어서, 직선으로는 잘 fitting 되지 않습니다. 2차원에서는 이렇기 시각화해서 확인할 수 있지만, 차원이 커질 수록 시각화 하기가 힘듭니다. 다음 단계에서는 learning curve를 사용하여 어떻게 모델을 조정시켜줄지 확인해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqI1G-5XVN-6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-873803433de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# add a columns of ones for the y-intercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainLinearReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinearRegCostFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_aug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#  Plot fit over the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# add a columns of ones for the y-intercept\n",
    "X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "theta = trainLinearReg(linearRegCostFunction, X_aug, y, lambda_=0)\n",
    "\n",
    "#  Plot fit over the data\n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1.5)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.plot(X, np.dot(X_aug, theta), '--', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpCl_Rz9nrCn"
   },
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 2 Bias-variance\n",
    "\n",
    "### 2.1 Learning Curves\n",
    "\n",
    "learning curve는 training set의 샘플 수(x축)에 따라 training error와 cross validation error(y축)를 시각화합니다. `learning curve` 함수는 training error와 cross validation error를 반환하는 함수입니다. \n",
    "\n",
    "X[:i, :] 와 y[:i]를 통해 i개의 샘플을 활용해서 i번째 training error를 계산해보세요. cross validation error는 cross validation set에 있는 모든 샘플들을 활용해 계산하세요. \n",
    "\n",
    "theta를 구하기 위해서는 위에서 사용한 `utils.trainLinearReg()`함수를 사용하세요.\n",
    "\n",
    "training error는 다음과 같이 정의됩니다. \n",
    "\n",
    "$$ J_{\\text{train}} = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left(h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right] $$\n",
    "\n",
    "여기서는 regularized term이 없는 것에 주의하세요. 즉 기존에 구축한 `linearRegCostFunction` 함수를 사용할 것이라면, lambda_를 0으로 줘야 합니다. \n",
    "\n",
    "i 번째 training error는 i개의 샘플에 대해서 계산하고, 반면 cross validation error는 항상 모든 cross validation set의 샘플들에 대해서 계산하는거 잊지마세요!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geCmzktapzAP"
   },
   "outputs": [],
   "source": [
    "# 앞서 정의한 linearRegCostFunction를 통해 J 값을 구해보겠습니다\n",
    "# X[:5, :], y[:5]에 써보세요! lambda 값이 0이여야 합니다\n",
    "J_exer = None\n",
    "J_exer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uv9aFveDnrCo"
   },
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, lambda_=0):\n",
    "    \"\"\"\n",
    "    training error와 cross validation error를 반환하는 함수 \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        (m x n + 1) 크기의 행렬. m은 sample의 개수이고, n은 \n",
    "        상수항을 더하기 이전의 feature의 개수 \n",
    "        \n",
    "    y : array_like\n",
    "        각 sample들의 label값\n",
    "        크기는 (m, )\n",
    "        \n",
    "    Xval : array_like\n",
    "        (m_val x n + 1) 크기의 행렬. m_val은 sample의 개수이고, n은 \n",
    "        상수항을 더하기 이전의 feature의 개수 \n",
    "        \n",
    "    yval : array_like\n",
    "        각 validation sample들의 label값\n",
    "        크기는 (m_val, )\n",
    "        \n",
    "    lambda_ : float, optional\n",
    "        regularization 파라미터 \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    error_train : array_like\n",
    "        크기 m인 벡터. \n",
    "        \n",
    "    error_val : array_like\n",
    "        크기 m인 벡터. \n",
    "\n",
    "    Instructions\n",
    "    ------------\n",
    "    error_train[i-1] = i개의 training sample을 통해 학습한 모델의 training error\n",
    "    error_val[i-1] = i개의 training sample을 통해 학습한 모델의 cross validation error\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    - linearRegCostFunction을 활용해서 구현한다면은, training error와 cross validation error를 구할 때는\n",
    "    lambda_값을 0으로 두세요. 반면, theta를 구할 때는 lambda_값을 그대로 반영해야 합니다. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    다음의 의사코드를 활용해보세요\n",
    "     \n",
    "           for i in range(1, m+1):\n",
    "               # 1. X[:i, :]와 y[:i], 그리고 lambda_를 활용해서 theta 구하기, (utils.trainLinearReg() 함수 사용하기)\n",
    "               # 2. training error, cross validation error 구하기\n",
    "               # 3. error_train[i-1] 그리고 error_val[i-1]에다가 구한 값 저장하기 \n",
    "               ....  \n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "\n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m)\n",
    "    error_val   = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "         \n",
    "    for i in range(1, m+1):    \n",
    "        # i개의 sample로 theta 학습\n",
    "        theta = None\n",
    "\n",
    "        # 학습된 theta로 예측값 산출하고, 실제값과 차이\n",
    "        error_train[i-1] = None\n",
    "        error_val[i-1] = None\n",
    "        \n",
    "    # =============================================================\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zh1eDHmMnrCq"
   },
   "source": [
    "아래 셀을 실행시키면, 다음과 같은 그림이 나와야 할 것입니다. \n",
    "\n",
    "![](Figures/learning_curve.png)\n",
    "\n",
    "위와 같은 그림은, 모델의 bias가 클 경우 나타납니다. 즉 우리의 linear regression 모델이 너무 간단하기 때문에 data에 잘 fitting되지 않는 거죠. 다음 단계에서는 polynomial regression을 통해 data에 더 잘 fitting되는 모델을 찾아 볼 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMPkwA9XnrCq",
    "outputId": "28cbd129-ecc8-4478-aa1a-e5d8429054fc"
   },
   "outputs": [],
   "source": [
    "X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "Xval_aug = np.concatenate([np.ones((yval.size, 1)), Xval], axis=1)\n",
    "error_train, error_val = learningCurve(X_aug, y, Xval_aug, yval, lambda_=0)\n",
    "\n",
    "pyplot.plot(np.arange(1, m+1), error_train, np.arange(1, m+1), error_val, lw=2)\n",
    "pyplot.title('Learning curve for linear regression')\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 150])\n",
    "\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIXItaN8nrCs"
   },
   "source": [
    "답안을 제출해보세요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgW0P8XZnrCt"
   },
   "outputs": [],
   "source": [
    "submit(learningCurve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUQ5L4FnnrCu"
   },
   "source": [
    "## 3 Polynomial regression\n",
    "\n",
    "앞서 linear regression의 문제는 feature수가 너무 적다는 것이였죠. 그에 반면\n",
    "polynomial regression의 hypothesis는 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x)  &= \\theta_0 + \\theta_1 \\times (\\text{waterLevel}) + \\theta_2 \\times (\\text{waterLevel})^2 + \\cdots + \\theta_p \\times (\\text{waterLevel})^p \\\\\n",
    "& = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_p x_p\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " $x_1 = (\\text{waterLevel})$, $x_2 = (\\text{waterLevel})^2$ , $\\cdots$, $x_p =\n",
    "(\\text{waterLevel})^p$, 이런식으로 생각을 해본다면은, 우리가 위에서 사용한 함수들을 그대로 적용할 수 있는 간단한 multiple linear regression 문제가 됩니다. \n",
    "\n",
    "데이터셋의 feature를 이렇게 늘리는 함수인 `polyFeatures`를 만들어 봅시다. 해당 함수는 크기 $m \\times 1$인 training set $X$ 를 $m \\times p$ 인 `X_poly`로 만들어 줍니다. i번째 column은 X를 i번 제곱한 값을 지니게 되는거죠.\n",
    "\n",
    "np.power() 함수를 사용해서 문제를 풀어보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykX-2kDhvFAr"
   },
   "outputs": [],
   "source": [
    "# np.power() 함수는 array를 받아 array의 각 요소를 지정 횟수만큼 제곱시켜주는 함수입니다\n",
    "A = np.array([2, 3, 5])\n",
    "np.power(A, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEtR3CCMtZA2"
   },
   "outputs": [],
   "source": [
    "# 현재 column 수가 1개인 X가 있습니다.\n",
    "# X_poly에 x1 = x, x2 = x^3 인 컬럼을 2개 가지는 변수를 할당해 보세요\n",
    "X_poly = np.zeros((X.shape[0], 2))\n",
    "X_poly[:, 0] = None\n",
    "X_poly[:, 1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lsm14PBNnrCv"
   },
   "outputs": [],
   "source": [
    "def polyFeatures(X, p):\n",
    "    \"\"\"\n",
    "    p승 만큼 데이터를 뻥튀기 시킨다. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        크기 m인 벡터. m은 training sample의 개수 \n",
    "    \n",
    "    p : int\n",
    "        뻥튀기 하고자 하는 power 값\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    X_poly : array_like\n",
    "        크기 m x p인 행렬 \n",
    "\n",
    "        각 행의 값을 확인해보면 다음과 같음\n",
    "        \n",
    "        X_poly[i, :] = [X[i], X[i]**2, X[i]**3 ...  X[i]**p]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X_poly = np.zeros((X.shape[0], p))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    for i in range(p) :\n",
    "\n",
    "        X_poly[:, i] = None\n",
    "\n",
    "    # ============================================================\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBN2AMannrCx"
   },
   "source": [
    "해당 함수를 training set, cross validation set, test set에 적용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ximdr6sEnrCx"
   },
   "outputs": [],
   "source": [
    "p = 8\n",
    "\n",
    "# feature 증가 시키고 normalize 해주기\n",
    "X_poly = polyFeatures(X, p)\n",
    "X_poly, mu, sigma = featureNormalize.test(X_poly)\n",
    "X_poly = np.concatenate([np.ones((m, 1)), X_poly], axis=1)\n",
    "\n",
    "# feature 증가 시키고 normalize 해주기\n",
    "X_poly_test = polyFeatures(Xtest, p)\n",
    "X_poly_test -= mu\n",
    "X_poly_test /= sigma\n",
    "X_poly_test = np.concatenate([np.ones((ytest.size, 1)), X_poly_test], axis=1)\n",
    "\n",
    "# feature 증가 시키고 normalize 해주기\n",
    "X_poly_val = polyFeatures(Xval, p)\n",
    "X_poly_val -= mu\n",
    "X_poly_val /= sigma\n",
    "X_poly_val = np.concatenate([np.ones((yval.size, 1)), X_poly_val], axis=1)\n",
    "\n",
    "print('Normalized Training Example 1:')\n",
    "X_poly[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2ca3byVnrCz"
   },
   "source": [
    "답안을 제출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIg-vVhwnrC0"
   },
   "outputs": [],
   "source": [
    "submit(polyFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgchwFRInrC3"
   },
   "source": [
    "## 3.1 Learning Polynomial Regression\n",
    "\n",
    "이제 theta를 학습시킨뒤 lambda가 0인 상태에서 시각화를 하게 되면 다음과 같은 그래프가 나옵니다.:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/polynomial_regression.png\"></td>\n",
    "        <td><img src=\"Figures/polynomial_learning_curve.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "training set에 정말 잘 fitting되는 것을 확인할 수 있습니다. 허나 cross validation set에 대해서는 error가 감소하다가 증가하는 것을 볼 수 있습니다. \n",
    "즉 training set에 overfitting 되었다는 의미인거죠. 또한 training error와 cross validation error 사이에 큰 gap이 있습니다. 이것은 high variance가 문제라는 뜻입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CtM2VNjnrC4"
   },
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "theta = trainLinearReg(linearRegCostFunction, X_poly, y,\n",
    "                             lambda_=lambda_, maxiter=55)\n",
    "\n",
    "# Plot training data and fit\n",
    "pyplot.plot(X, y, 'ro', ms=10, mew=1.5, mec='k')\n",
    "\n",
    "plotFit(polyFeatures, np.min(X), np.max(X), mu, sigma, theta, p)\n",
    "\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.title('Polynomial Regression Fit (lambda = %f)' % lambda_)\n",
    "pyplot.ylim([-20, 50])\n",
    "\n",
    "pyplot.figure()\n",
    "error_train, error_val = learningCurve(X_poly, y, X_poly_val, yval, lambda_)\n",
    "pyplot.plot(np.arange(1, 1+m), error_train, np.arange(1, 1+m), error_val)\n",
    "\n",
    "pyplot.title('Polynomial Regression Learning Curve (lambda = %f)' % lambda_)\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 100])\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "\n",
    "print('Polynomial Regression (lambda = %f)\\n' % lambda_)\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wheTGsx-nrC6"
   },
   "source": [
    "overfitting 문제, 즉 high variance 문제를 해결하기 위한 방법 중에 regularization term을 활용하는 방법이 있습니다. 아래 내용을 참고하세요. \n",
    "\n",
    "### 3.2 Optional (ungraded) exercise: Adjusting the regularization parameter\n",
    "\n",
    "위의 셀에서 lambda_를 1 그리고 100으로 두고 각각 셀을 실행시켜 보세요. lambda_를 1로 두게 되면 fitting도 잘되고, learning curve도 비교적 적은 값으로 수렴하는 것을 볼 수 있을 것입니다. 즉 bias와 variance 간의 trade off가 잘 이루어 졌다는 것입니다. 아래의 그림과 같은 그래프가 나올 것입니다. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/polynomial_regression_reg_1.png\"></td>\n",
    "        <td><img src=\"Figures/polynomial_learning_curve_reg_1.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "반면 lambda_가 100일 때는 fitting이 잘 안된 것을 볼 수 있습니다. 즉 패널티가 너무 많이 반영되었다는 뜻이죠. \n",
    "\n",
    "![](Figures/polynomial_regression_reg_100.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGPDhXUynrC8"
   },
   "source": [
    "### 3.3 Selecting $\\lambda$ using a cross validation set\n",
    "\n",
    "이제 lambda를 결정하기 위한 함수를 구현해볼 것입니다. lambda가 변할 때마다 training set error와 cross validation error가 어떻게 변하는지 확인할 것입니다. \n",
    "\n",
    "`validationCurve` 함수는 사용한 lambda 값과, lambda값 별 training error와 cross validation error를 반환하는 함수입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cjq4Dt2nrC9"
   },
   "outputs": [],
   "source": [
    "def validationCurve(X, y, Xval, yval):\n",
    "   \n",
    "    # 사용할 lambda 값들 \n",
    "    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "    error_train = np.zeros(len(lambda_vec))\n",
    "    error_val = np.zeros(len(lambda_vec))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    for i in range(len(lambda_vec)):\n",
    "        \n",
    "        lambda_ = lambda_vec[i]\n",
    "        \n",
    "        theta = trainLinearReg(None, None, None, None) \n",
    "        \n",
    "        # 학습된 theta로 예측값 산출하고, 실제값과 차이 계산\n",
    "        # linearRegCostFunction()을 활용해보세요\n",
    "        \n",
    "        error_train[i], _ = None\n",
    "        \n",
    "        error_val[i], _ = None\n",
    "\n",
    "    # ============================================================\n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJtMVEWnnrDB"
   },
   "source": [
    "아래 셀을 실행시키면 다음과 같은 그래프가 나올 것입니다. \n",
    "\n",
    "![](Figures/cross_validation.png)\n",
    "\n",
    "그래프를 확인해보면, cross validation error가 가장 작은 lambda의 값은 3인 것을 확인 할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationCurve(X, y, Xval, yval):\n",
    "\n",
    "    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "    error_train = np.zeros(len(lambda_vec))\n",
    "    error_val = np.zeros(len(lambda_vec))\n",
    "\n",
    "    for i in range(len(lambda_vec)):\n",
    "        lambda_ = lambda_vec[i]\n",
    "        theta = trainLinearReg(linearRegCostFunction, X, y, lambda_)\n",
    "        error_train[i], _ = linearRegCostFunction(X, y, theta, 0)\n",
    "        error_val[i], _ = linearRegCostFunction(Xval, yval, theta, 0)\n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qqDMu74nrDB"
   },
   "outputs": [],
   "source": [
    "lambda_vec, error_train, error_val = validationCurve(X_poly, y, X_poly_val, yval)\n",
    "\n",
    "pyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('lambda')\n",
    "pyplot.ylabel('Error')\n",
    "\n",
    "print('lambda\\t\\tTrain Error\\tValidation Error')\n",
    "for i in range(len(lambda_vec)):\n",
    "    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCgNine7nrDD"
   },
   "source": [
    "결과를 제출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ1kDTq6nrDD"
   },
   "outputs": [],
   "source": [
    "submit(validationCurve)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "도와주세요형님들.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

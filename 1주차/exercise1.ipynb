{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtX3UVUbPZjT"
   },
   "source": [
    "# 1주차 실습\n",
    "## 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTmzQCEkPZjV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # 벡터 계산을 위한 라이브러리 \n",
    "from matplotlib import pyplot # 시각화를 위한 라이브러리\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# 제출을 하기 위한 라이브러리, exercise1_kor.ipynb가 위치한 폴더에 같이 위치해 있음 \n",
    "import utils \n",
    "\n",
    "# 채점하는 객체 불러오기\n",
    "grader = utils.Grader()\n",
    "\n",
    "# matplotlib 한테 결과물을 노트북에 새기도록 명시\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAQyBlVSPZja"
   },
   "source": [
    "## 제출과 채점\n",
    "\n",
    "문제를 풀고나서는, grader 객체를 통해 문제를 제출 해주세요.\n",
    "\n",
    "진행 순서는 다음과 같습니다.\n",
    "\n",
    "문제별 점수는 아래 채점표를 확인해주세요. \n",
    "\n",
    "**1주차**\n",
    "\n",
    "| Section | Part                                           |Submitted Function                     | Points \n",
    "|---------|:-                                             |:-                                     | :-:    \n",
    "| 1       | [Warm up exercise](#section1)                  | [`warmUpExercise`](#warmUpExercise)    |  10    \n",
    "| 2       | [Compute cost for one variable](#section2)     | [`computeCost`](#computeCost)         |  40    \n",
    "| 3       | [Gradient descent for one variable](#section3) | [`gradientDescent`](#gradientDescent) |  50    \n",
    "|         | Total Points                                   |                                       | 100    \n",
    "\n",
    "\n",
    "답안지 제출은 여러번 가능합니다. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "답안지 수정을 한 후에는, 꼭 답안지 제출하는 코드를 다시 한번 더 실행 시켜 주세요.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yh28r7jPZjc"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1 Simple python and `numpy` function\n",
    "\n",
    "다음 코드를 활용하여, 아래에 있는 문제를 해결해 보세요.\n",
    "\n",
    "```python\n",
    "A = np.eye(5)\n",
    "```\n",
    "<a id=\"warmUpExercise\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opMXcfgwPZjd"
   },
   "outputs": [],
   "source": [
    "def warmUpExercise():\n",
    "    \"\"\"\n",
    "    단위 행렬을 계산하는 함수\n",
    "    \n",
    "    반환 값\n",
    "    -------\n",
    "    A : array 형태의\n",
    "        5x5 단위 행렬\n",
    "    \n",
    "    문제\n",
    "    ------------\n",
    "    5x5 단위 행렬을 반환 하도록 코드를 완성하세요.\n",
    "    \"\"\"    \n",
    "    # ======== YOUR CODE HERE ======\n",
    "    \n",
    "    A = \n",
    "    \n",
    "    # ==============================\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOecICwCPZji"
   },
   "source": [
    "아래에 있는 셀을 실행시키면, 다음과 같은 결과가 나올 것입니다. \n",
    "\n",
    "```python\n",
    "array([[ 1.,  0.,  0.,  0.,  0.],\n",
    "       [ 0.,  1.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  1.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  1.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7j8zzhrPZjn"
   },
   "outputs": [],
   "source": [
    "warmUpExercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuG_-cXiPZjr"
   },
   "source": [
    "### 1.1 Submitting solutions\n",
    "\n",
    "아래 셀을 실행시켜서 coursera에 답안지를 제출 해보세요. \n",
    "\n",
    "cousera 이메일과, 과제 링크에 들어가면 확인 할 수 있는 고유 토큰 번호를 입력해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jejzS_plPZjt"
   },
   "outputs": [],
   "source": [
    "# 위에서 정의한 함수를, 채점 객체의 part 1에다가 할당시키기\n",
    "grader.answer[0] = warmUpExercise\n",
    "\n",
    "# 추가된 함수를 coursera grader에게 보내서, 채점 받기\n",
    "grader.grade(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olS-2tDyPZjw"
   },
   "source": [
    "## 2 Linear regression with one variable\n",
    "\n",
    "용선이는 TNT 푸드트럭 CEO 입니다. 용선이는 새로운 체인점을 열기 위해 여러 개의 도시를 물색하고 있습니다. 체인점은 이미 여러 도시에 분포되어 있고, 해당 도시의 인구 수와 체인점의 수익에 대한 데이터를 용선이는 지니고 있습니다. 용선이는 이 데이터를 활용하여, 다음 체인점은 어느 도시에 개점할 지 결정하고자 합니다. \n",
    "\n",
    "`Data/ex1data1.txt` 파일은 우리의 문제에 대한 데이터를 지니고 있습니다. 첫번째 column은 도시의 인구 데이터 입니다. (10,000명 단위) 두번째 column은 해당 도시 푸드 트럭의 수익 데이터 입니다. (10,000 달러 단위) 두번째 column의 음수 값은, 손해를 의미합니다. \n",
    "\n",
    "아래 코드를 실행해서 데이터를 불러오고, X와 y 변수에 각각의 column들을 저장해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ystCPD_KPZjx"
   },
   "outputs": [],
   "source": [
    "# csv 파일 불러오기\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data1.txt'), delimiter=',')\n",
    "X, y = data[:, 0], data[:, 1]\n",
    "\n",
    "m = y.size  # training 인스턴스의 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DOyqK3F_sTu"
   },
   "outputs": [],
   "source": [
    "# 데이터 형태 확인해보기 \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe1G2_u2SJ74"
   },
   "outputs": [],
   "source": [
    "# 상위 5개 데이터 확인해보기\n",
    "data[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pECyk4YPZj0"
   },
   "source": [
    "### 2.1 Plotting the Data\n",
    "\n",
    "데이터를 시각화 한다면, 직관적으로 이해하기가 편합니다. 방금 불러온 데이터는 산점도를 활용해서 시각화할 수 있습니다. 왜냐하면 오직 두가지의 feature만 지니고 있기 때문이죠.(수익과 인구) 하지만 대부분의 데이터셋 들은 multi-dimensional 이라서, 2-d plot으로는 시각화 할 수 없습니다. \n",
    "\n",
    "Python에는 다양한 시각화 라이브러리들이 존재합니다. 이 블로그를 참조해보세요. \n",
    "[blog post](https://blog.modeanalytics.com/python-data-visualization-libraries/)\n",
    "\n",
    "이 강의에서는 matploblib를 사용해서 시각화를 할 것입니다. matplotlib는 python에서 유명한 시각화 라이브러리 중 하나이며, matploblib 안에 있는 pyplot 모듈은 아주 편리한 인터페이스를 제공합니다. 이것은 MATLAB의 시각화 인터페이스를 따라한 것입니다. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "참고로 여기에서는 'from matplotlib import pyplot' 명령어를 사용해서 pyplot을 가지고 올 것입니다. 아마 다른 파이썬 코드에서는 'import matplotlib.pyplot as plt'라고 쓰는 것을 확인 할 수 있을 것입니다. 하지만 여기에서는 plt라는 줄임말을 쓰지 않고 pyplot이라고 사용할 것입니다. 이 부분 참고해주세요.\n",
    "</div>\n",
    "\n",
    "아래의 코드를 활용해서 'plotData' 함수를 완성해보세요. \n",
    "\n",
    "```python\n",
    "    pyplot.plot(x, y, 'ro', ms=10, mec='k')\n",
    "    pyplot.ylabel('Profit in $10,000')\n",
    "    pyplot.xlabel('Population of City in 10,000s')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIUikyxCPZj1"
   },
   "outputs": [],
   "source": [
    "def plotData(x, y):\n",
    "    \"\"\"\n",
    "    이 함수는 x와 y 데이터를 시각화 하는 함수입니다. 해당 데이터를 \n",
    "    시각화 하고, x축과 y축에 라벨을 붙여 보세요. \n",
    "    \n",
    "    파라미터\n",
    "    ----------\n",
    "    x : x_axis를 위한 array 형태의 data\n",
    "\n",
    "    y : y_axis를 위한 array 형태의 data\n",
    "        \n",
    "        주의: x와 y는 크기가 같아야 합니다. \n",
    "    \n",
    "    문제\n",
    "    ------------\n",
    "    \n",
    "    figure 함수와 plot 함수를 활용해서 training data를 시각화해 보세요. \n",
    "    xlabel과 ylabel 함수를 활용하여 axis의 라벨값을 붙여 보세요. \n",
    "    도시 인구수와 푸드트럭 수익에 관한 데이터가 각각 x와 y 인자에 할당 되었다고\n",
    "    가정하세요. \n",
    "    \n",
    "    힌트\n",
    "    ----\n",
    "    'ro'는 \"red o\", 즉 빨간색 동그라미를 의미합니다. \n",
    "    또한, ms 인자의 숫자값을 조정함으로써 동그라미의 크기를 조절할 수 있습니다. \n",
    "    마지막으로, 'mec' 인자를 활용하여 동그라미의 테두리의 색을 선택할 수 있습니다. \n",
    "    커서를 pyplot.plot() 괄호 사이에 둔다음에, tab을 눌러 'mec'이 받는 인자값들을 \n",
    "    확인해보세요.\n",
    "    \"\"\"\n",
    "    fig = pyplot.figure()  # open a new figure\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================= \n",
    "    \n",
    "   \n",
    "\n",
    "    # =============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vo6W69WhPZj3"
   },
   "source": [
    "Now run the defined function with the loaded data to visualize the data. The end result should look like the following figure:\n",
    "\n",
    "![](Figures/dataset1.png)\n",
    "\n",
    "다음 함수를 실행시켜서 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IPn-bW6PZj4"
   },
   "outputs": [],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5wArpucPZj-"
   },
   "source": [
    "pyplot.plot에 대한 설명을 새로운 창에다가 열어서 보고 싶다면, 아래에 있는 셀을 실행시켜 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDPht_CzPZkB"
   },
   "outputs": [],
   "source": [
    "?pyplot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVwiwEBqPZkG"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 2.2 Gradient Descent\n",
    "\n",
    "여기에서는, gradient descent를 활용해서 linear regression의 파라미터를 학습시킬 것입니다. \n",
    "\n",
    "#### 2.2.1 Update Equations\n",
    "\n",
    "linear regression의 목적은 cost function(비용함수)를 최소화 시키는 것입니다. \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "여기서 가설 $h_\\theta(x)$ 은 다음과 같습니다. \n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$\n",
    "\n",
    "linear regression 모델의 파라미터는 $\\theta_j$ 값들입니다. 해당 값들을 조정함으로써, 비용함수인 $J(\\theta)$을 최소화 시킵니다. 파라미터를 조정 하는 방법 중에 batch gradient descent가 있습니다. BGD에서는 반복할 때 마다 다음 과정을 실시합니다. \n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{ 모든 } j \\text{에 대해서 동시에 } \\theta_j \\text{ 를 업데이트 시킨다 }  $$\n",
    "\n",
    "gradient descent를 한 스텝씩 시행 할 때마다, 파라미터인 $\\theta_j$는 J($\\theta$)를 최소화 시키는 최적의 값으로 수렴 할 것입니다. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**참고** 우리는 각각의 sample들을 $X$ 변수의 row로 저장을 해두었습니다. intercept 부분인 ($\\theta_0$)를 고려하기 위해서 $X$ 에다가 column을 앞쪽에 추가하고, 1로 채워봅시다. 이 방법을 통해 우리는 $\\theta_0$를 하나의 'feature'로 취급 할 수 있습니다. \n",
    "</div>\n",
    "\n",
    "\n",
    "#### 2.2.2 Implementation\n",
    "\n",
    "아래 셀을 실행시켜서, intercept 부분인 $\\theta_0$을 고려하기 위해 column을 추가해 봅시다. 아래 셀은 한번만 실행 시켜 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MY-6YEeGPZkK"
   },
   "outputs": [],
   "source": [
    "# X 변수에다가, 1로 채워진 column을 추가해보겠습니다. \n",
    "# np.ones(m)은 m개 만큼의 1로 채워진 array를 만드는 함수입니다. 여기서 m은 training set의 샘플 수를 의미합니다. \n",
    "# np.stack()은 axis 방향에 따라 array를 합치는 함수입니다.\n",
    "# axis=0은 row, axis=1은 열을 뜻합니다. \n",
    "X = np.stack([np.ones(m), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNDmQQODL_q0"
   },
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRLiWSYuPZkO"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "#### 2.2.3 Computing the cost $J(\\theta)$\n",
    "\n",
    "Gradient Descent를 활용하면서 cost function인 $J(\\theta)$를 최소화 할 때, cost를 계산해보면서 수렴하는 것을 지켜 보는 것이 도움이 될 것입니다. 이 부분에서는 cost 값이 0으로 수렴하는 것을 확인할 수 있게 $J(\\theta)$를 계산하는 함수를 만들어 볼 것입니다. \n",
    "\n",
    "<a id=\"computeCost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTBsrhw3PZkP"
   },
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "    linear regression의 cost를 계산하는 함수입니다. theta를 linear regression의 \n",
    "    파라미터로 사용하고, \n",
    "    Compute cost for linear regression. Computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    파라미터\n",
    "    ----------\n",
    "    X : array 형태\n",
    "        training dataset의 feature들을 의미하며, 크기는 (m, n+1) 입니다. \n",
    "        여기서 m은 sample의 수이고, n은 feature의 수를 의미합니다. \n",
    "        우리가 1로 채워진 벡터를 추가 했으므로, 여기에서는 n+1개의 column이 \n",
    "        나오게 됩니다. \n",
    "    \n",
    "    y : array 형태\n",
    "        training dataset의 실제 라벨 값을 의미합니다. 크기는 (m, ) 입니다. \n",
    "    \n",
    "    theta : array 형태\n",
    "        linear regression의 파라미터입니다. 크기는 (n+1, ) 입니다. \n",
    "    \n",
    "    반환 값\n",
    "    -------\n",
    "    J : float 형태\n",
    "        cost function의 값을 반환 합니다. \n",
    "    \n",
    "    문제\n",
    "    ------------\n",
    "    특정한 조합의 theta의 비용을 계산해보세요.\n",
    "    J 변수에 비용을 저장해서 반환 하세요. \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE =====================\n",
    "    \n",
    "    J = \n",
    "    \n",
    "    # ===========================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmQOF3HePZkS"
   },
   "source": [
    "아래 셀을 실행 시켜서 함수가 잘 구현 되었는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RshcUZAPZkT"
   },
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta=np.array([0.0, 0.0]))\n",
    "print('theta 값 = [0, 0] \\n계산된 cost = %.2f' % J)\n",
    "print('나와야 하는 cost 값 32.07\\n')\n",
    "\n",
    "# further testing of the cost function\n",
    "J = computeCost(X, y, theta=np.array([-1, 2]))\n",
    "print('theta 값 = [-1, 2] \\n계산된 cost = %.2f' % J)\n",
    "print('나와야 하는 cost 값 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEOmw6gkPZkV"
   },
   "source": [
    "아래 셀을 실행 시켜서, 답안지를 제출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20iDeLzKPZkW"
   },
   "outputs": [],
   "source": [
    "grader.answer[1] = computeCost\n",
    "grader.grade(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAOVEhZhPZkY"
   },
   "source": [
    "<a id=\"section3\"></a>\n",
    "#### 2.2.4 Gradient descent\n",
    "\n",
    "이제 gradient descent를 실시하는 함수를 만들 것입니다. \n",
    "num_iters 만큼 반복하는 for문은 이미 구현되어 있으니, 매번 반복 할 때마다 $\\theta$를 업데이트 하는 코드만 구현하면 됩니다. \n",
    "\n",
    "함수를 구현할 때, 무엇을 최적화하고자 하는지, 그리고 무엇이 업데이트 되는 것인지 머리에 염두해두고 프로그래밍을 하세요. \n",
    "\n",
    "비용함수인 $J(\\theta)$의 결과물은 $\\theta$에 의해 값이 변하게 됩니다. $X$ and $y$에 따라 값이 변하게 되는 것이 아닙니다. \n",
    "\n",
    "Gradient Descent가 제대로 실행 되고 있는지 알기 위해선, $J(\\theta)$ 값이 매번 반복 할 때마다 감소하고 있는지 확인하는 것이 좋습니다. \n",
    "\n",
    "`gradientDescent` 함수는 매번 반복 할 때 마다 cost 값을 list에 저장하도록 구현되었습니다. 구현이 정상적으로 되었다면, $J(\\theta)$ 값은 절대로 증가하면 안됩니다. 반복이 끝날 쯤에는, 값이 수렴하는 모습을 보여야 할 것입니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eic_249OPZka"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    gradient descent를 통해 `theta`를 학습시킵니다. `num_iters` 만큼의\n",
    "    gradient steps를 통해 theta를 업데이트 시킵니다. 이 때, learning rate는\n",
    "    `alpha`입니다. \n",
    "    \n",
    "    파라미터\n",
    "    ----------\n",
    "    X : array 형태\n",
    "        training set의 feature들. 크기는 (m x n+1).\n",
    "    \n",
    "    y : array 형태\n",
    "        training set의 label을 의미. 크기는 (m, ).\n",
    "    \n",
    "    theta : array 형태\n",
    "        linear regression 파라미터의 초기값.\n",
    "        크기는 (n+1, ).\n",
    "    \n",
    "    alpha : float 형태\n",
    "        learning rate.\n",
    "    \n",
    "    num_iters : int 형태\n",
    "        gradient descent를 실시할 반복횟수\n",
    "    \n",
    "    반환 값\n",
    "    -------\n",
    "    theta : array 형태\n",
    "        학습된 linear regression 파라미터. 크기는 (n+1, ).\n",
    "    \n",
    "    J_history : list 형태\n",
    "        매번 반복 할 때마다의 cost function의 값을 기록한 list \n",
    "    \n",
    "    문제\n",
    "    ------------\n",
    "    \n",
    "    파라미터인 theta에 대해 한번의 gradient step을 실시하도록 설계해보세요.\n",
    "    \n",
    "    디버깅을 할 때, computeCost 함수를 활용하여 cost 값과 gradient를 출력하는 것이\n",
    "    도움 될 것입니다. \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # theta의 초기값을 변화시키지 않도록, theta를 복사해서, 함수 내에서 만 \n",
    "    # 적용되는 새로운 theta 변수를 만들어 봅시다. \n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = [] # 매번 반복할 때마다 cost를 여기다가 저장하세요. \n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # ==================== YOUR CODE HERE =================================\n",
    "\n",
    "        \n",
    "        # =====================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V161NlNYPZkd"
   },
   "source": [
    "아래 셀을 실행 시켜서, 함수가 잘 구현 되었는지 확인해보세요. \n",
    "theta의 초기값은 0으로, learning rate는 0.01로 실시 해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDwynL_MPZke"
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "theta = np.zeros(2)\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)\n",
    "print('gradient descent를 통해 최종적으로 업데이트 된 theta: {:.4f}, {:.4f}'.format(*theta))\n",
    "print('나와야 하는 theta: [-3.6303, 1.1664]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "korjqysXPZkh"
   },
   "source": [
    "이제 학습된 파라미터를 통해 선을 그려봅시다. 다음과 같이 나와야 합니다. \n",
    "\n",
    "![](Figures/regression_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1568035083413,
     "user": {
      "displayName": "안성진",
      "photoUrl": "",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "cSA5VdifPZki",
    "outputId": "1282930d-7378-485f-df13-965b06cdd0d5"
   },
   "outputs": [],
   "source": [
    "# 선 그리기\n",
    "plotData(X[:, 1], y)\n",
    "pyplot.plot(X[:, 1], np.dot(X, theta), '-')\n",
    "pyplot.legend(['Training data', 'Linear regression']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LhD0r82PZkl"
   },
   "source": [
    "이제 학습된 파라미터를 가지고, 인구수가 35,000일 때와 70,000일 때의 기대 되는 수익을 계산해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3yBRNI4PZkm"
   },
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = np.dot([1, 3.5], theta)\n",
    "print('인구 수 = 35,000 일 때, 기대 되는 수익은 {:.2f}\\n'.format(predict1*10000))\n",
    "\n",
    "predict2 = np.dot([1, 7], theta)\n",
    "print('인구 수 = 70,000 일 때, 기대 되는 수익은 {:.2f}\\n'.format(predict2*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WETySZdpPZkt"
   },
   "source": [
    "아래 셋을 실행 시켜서 답안지를 제출해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcL79MJPPZkt"
   },
   "outputs": [],
   "source": [
    "grader.answer[2] = gradientDescent\n",
    "grader.grade(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJoOkqZIPZkw"
   },
   "source": [
    "### 2.4 Visualizing $J(\\theta)$\n",
    "\n",
    "이제 $J(\\theta)$를 보다 더 잘 이해하기 위해서 $\\theta_0$ 와 $\\theta_1$ 값을 활용해서 시각화를 해봅시다. 여기서는 코딩을 할 필요는 없지만, 해당 코드가 무슨 역할을 하는지는 인지 할 필요가 있습니다. \n",
    "\n",
    "아래 셀은, 우리가 명시한 theta 값들을 `computeCost` 함수의 인자로 받아서 cost를 계산하는 과정입니다. 해당 과정이 끝나면, $J(\\theta)$ 가 저장된 2-D array가 형성 될 것입니다. 그리고 나서 해당 값들은 matplotlib의 `plot_surface` 와 `contourf` 함수의 인자로 들어가 그래프를 형성 할 것입니다. 그래프는 다음과 같이 보여질 것입니다.\n",
    "\n",
    "![](Figures/cost_function.png)\n",
    "\n",
    "이 그래프의 목적은 $\\theta_0$ 와 $\\theta_1$가 변화할 때마다 $J(\\theta)$ 또한 변화나는 것을 보여주기 위함입니다. $J(\\theta)$은 그릇 모양이고, 보시다 시피 global minimum을 지니고 있습니다. 이 지점이 최적의 지점입니다. gradient descent를 실시 할 때마다 이 지점에 가깝에 위치 할 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4ZakrnOPZkx"
   },
   "outputs": [],
   "source": [
    "# J를 계산할 theta 조합들\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# 계산된 J를 저장 시킬 변수를 생성해두기, 초기값은 0으로\n",
    "J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n",
    "\n",
    "# J_vals 채우기\n",
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n",
    "        \n",
    "# surf 함수 자체 특성 때문에, J_vals를 transpose 시켜주기\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# surface plot 그리기\n",
    "fig = pyplot.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')\n",
    "pyplot.xlabel('theta0')\n",
    "pyplot.ylabel('theta1')\n",
    "pyplot.title('Surface')\n",
    "\n",
    "# contour plot 그리기\n",
    "ax = pyplot.subplot(122)\n",
    "pyplot.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))\n",
    "pyplot.xlabel('theta0')\n",
    "pyplot.ylabel('theta1')\n",
    "pyplot.plot(theta[0], theta[1], 'ro', ms=10, lw=2)\n",
    "pyplot.title('Contour, showing minimum')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT9-9S46PZk1"
   },
   "source": [
    "## Optional Exercises\n",
    "\n",
    "If you have successfully completed the material above, congratulations! You now understand linear regression and should able to start using it on your own datasets.\n",
    "\n",
    "For the rest of this programming exercise, we have included the following optional exercises. These exercises will help you gain a deeper understanding of the material, and if you are able to do so, we encourage you to complete them as well. You can still submit your solutions to these exercises to check if your answers are correct.\n",
    "\n",
    "## 3 Linear regression with multiple variables\n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file `Data/ex1data2.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price\n",
    "of the house. \n",
    "\n",
    "<a id=\"section4\"></a>\n",
    "### 3.1 Feature Normalization\n",
    "\n",
    "We start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1-jdmJIPZk1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "m = y.size\n",
    "\n",
    "# print out some data points\n",
    "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
    "print('-'*26)\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8GHQQbcPZk7"
   },
   "source": [
    "Your task here is to complete the code in `featureNormalize` function:\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
    "\n",
    "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min). In `numpy`, you can use the `std` function to compute the standard deviation. \n",
    "\n",
    "For example, the quantity `X[:, 0]` contains all the values of $x_1$ (house sizes) in the training set, so `np.std(X[:, 0])` computes the standard deviation of the house sizes.\n",
    "At the time that the function `featureNormalize` is called, the extra column of 1’s corresponding to $x_0 = 1$ has not yet been added to $X$. \n",
    "\n",
    "You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix $X$ corresponds to one feature.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n",
    "</div>\n",
    "<a id=\"featureNormalize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrc3wIFHPZk8"
   },
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    \"\"\"\n",
    "    Normalizes the features in X. returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    First, for each feature dimension, compute the mean of the feature\n",
    "    and subtract it from the dataset, storing the mean value in mu. \n",
    "    Next, compute the  standard deviation of each feature and divide\n",
    "    each feature by it's standard deviation, storing the standard deviation \n",
    "    in sigma. \n",
    "    \n",
    "    Note that X is a matrix where each column is a feature and each row is\n",
    "    an example. You needto perform the normalization separately for each feature. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    \"\"\"\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "\n",
    "    \n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSWa2vH3PZk-"
   },
   "source": [
    "Execute the next cell to run the implemented `featureNormalize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-RIdkgoPZlA"
   },
   "outputs": [],
   "source": [
    "# call featureNormalize on the loaded data\n",
    "X_norm, mu, sigma = featureNormalize(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzCTwyFoPZlC"
   },
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sexhnMCkPZlD"
   },
   "outputs": [],
   "source": [
    "grader[4] = featureNormalize\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNFZxTprPZlF"
   },
   "source": [
    "After the `featureNormalize` function is tested, we now add the intercept term to `X_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk2I0F2QPZlG"
   },
   "outputs": [],
   "source": [
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QekYFvu1PZlK"
   },
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 3.2 Gradient Descent\n",
    "\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix $X$. The hypothesis function and the batch gradient descent update\n",
    "rule remain unchanged. \n",
    "\n",
    "You should complete the code for the functions `computeCostMulti` and `gradientDescentMulti` to implement the cost function and gradient descent for linear regression with multiple variables. If your code in the previous part (single variable) already supports multiple variables, you can use it here too.\n",
    "Make sure your code supports any number of features and is well-vectorized.\n",
    "You can use the `shape` property of `numpy` arrays to find out how many features are present in the dataset.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** In the multivariate case, the cost function can\n",
    "also be written in the following vectorized form:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "          - (x^{(1)})^T - \\\\\n",
    "          - (x^{(2)})^T - \\\\\n",
    "          \\vdots \\\\\n",
    "          - (x^{(m)})^T - \\\\ \\\\\n",
    "        \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n",
    "\n",
    "the vectorized version is efficient when you are working with numerical computing tools like `numpy`. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent.\n",
    "</div>\n",
    "\n",
    "<a id=\"computeCostMulti\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBHpDn-ZPZlM"
   },
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression with multiple variables.\n",
    "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the cost function. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0\n",
    "    \n",
    "    # ======================= YOUR CODE HERE ===========================\n",
    "\n",
    "    \n",
    "    # ==================================================================\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfpvpeU5PZlQ"
   },
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVASznWfPZlS"
   },
   "outputs": [],
   "source": [
    "grader[5] = computeCostMulti\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SEsmCjyPZlV"
   },
   "source": [
    "<a id=\"gradientDescentMulti\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "siL6-Xd4PZlW"
   },
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "    Updates theta by taking num_iters gradient steps with learning rate alpha.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        A vector of shape (m, ) for the values at a given data point.\n",
    "    \n",
    "    theta : array_like\n",
    "        The linear regression parameters. A vector of shape (n+1, )\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate for gradient descent. \n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations to run gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector theta.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    # make a copy of theta, which will be updated by gradient descent\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # ======================= YOUR CODE HERE ==========================\n",
    "\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(computeCostMulti(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wd5b9g_LPZlY"
   },
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXDWZ4eVPZlY"
   },
   "outputs": [],
   "source": [
    "grader[6] = gradientDescentMulti\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wf6YuAf_PZlb"
   },
   "source": [
    "#### 3.2.1 Optional (ungraded) exercise: Selecting learning rates\n",
    "\n",
    "In this part of the exercise, you will get to try out different learning rates for the dataset and find a learning rate that converges quickly. You can change the learning rate by modifying the following code and changing the part of the code that sets the learning rate.\n",
    "\n",
    "Use your implementation of `gradientDescentMulti` function and run gradient descent for about 50 iterations at the chosen learning rate. The function should also return the history of $J(\\theta)$ values in a vector $J$.\n",
    "\n",
    "After the last iteration, plot the J values against the number of the iterations.\n",
    "\n",
    "If you picked a learning rate within a good range, your plot look similar as the following Figure. \n",
    "\n",
    "![](Figures/learning_rate.png)\n",
    "\n",
    "If your graph looks very different, especially if your value of $J(\\theta)$ increases or even blows up, adjust your learning rate and try again. We recommend trying values of the learning rate $\\alpha$ on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on). You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** If your learning rate is too large, $J(\\theta)$ can diverge and ‘blow up’, resulting in values which are too large for computer calculations. In these situations, `numpy` will tend to return\n",
    "NaNs. NaN stands for ‘not a number’ and is often caused by undefined operations that involve −∞ and +∞.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**MATPLOTLIB tip:** To compare how different learning learning rates affect convergence, it is helpful to plot $J$ for several learning rates on the same figure. This can be done by making `alpha` a python list, and looping across the values within this list, and calling the plot function in every iteration of the loop. It is also useful to have a legend to distinguish the different lines within the plot. Search online for `pyplot.legend` for help on showing legends in `matplotlib`.\n",
    "</div>\n",
    "\n",
    "Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient descent takes a very long time to converge to the optimal value. Conversely, with a large learning rate, gradient descent might not converge or might even diverge!\n",
    "Using the best learning rate that you found, run the script\n",
    "to run gradient descent until convergence to find the final values of $\\theta$. Next,\n",
    "use this value of $\\theta$ to predict the price of a house with 1650 square feet and\n",
    "3 bedrooms. You will use value later to check your implementation of the normal equations. Don’t forget to normalize your features when you make this prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3ZlrfaDPZld"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instructions\n",
    "------------\n",
    "We have provided you with the following starter code that runs\n",
    "gradient descent with a particular learning rate (alpha). \n",
    "\n",
    "Your task is to first make sure that your functions - `computeCost`\n",
    "and `gradientDescent` already work with  this starter code and\n",
    "support multiple variables.\n",
    "\n",
    "After that, try running gradient descent with different values of\n",
    "alpha and see which one gives you the best result.\n",
    "\n",
    "Finally, you should complete the code at the end to predict the price\n",
    "of a 1650 sq-ft, 3 br house.\n",
    "\n",
    "Hint\n",
    "----\n",
    "At prediction, make sure you do the same feature normalization.\n",
    "\"\"\"\n",
    "# Choose some alpha value - change this\n",
    "alpha = 0.1\n",
    "num_iters = 400\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "# Plot the convergence graph\n",
    "pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "pyplot.xlabel('Number of iterations')\n",
    "pyplot.ylabel('Cost J')\n",
    "\n",
    "# Display the gradient descent's result\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ======================= YOUR CODE HERE ===========================\n",
    "# Recall that the first column of X is all-ones. \n",
    "# Thus, it does not need to be normalized.\n",
    "\n",
    "price = 0   # You should change this\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zCiWci3PZlf"
   },
   "source": [
    "*You do not need to submit any solutions for this optional (ungraded) part.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jd-okN8PPZlg"
   },
   "source": [
    "<a id=\"section7\"></a>\n",
    "### 3.3 Normal Equations\n",
    "\n",
    "In the lecture videos, you learned that the closed-form solution to linear regression is\n",
    "\n",
    "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent. \n",
    "\n",
    "First, we will reload the data to ensure that the variables have not been modified. Remember that while you do not need to scale your features, we still need to add a column of 1’s to the $X$ matrix to have an intercept term ($\\theta_0$). The code in the next cell will add the column of 1’s to X for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_gJ22ExPZlh"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "m = y.size\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mPPenAkPZlk"
   },
   "source": [
    "Complete the code for the function `normalEqn` below to use the formula above to calculate $\\theta$. \n",
    "\n",
    "<a id=\"normalEqn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3e_TpMklPZll"
   },
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution to linear regression using the normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        The value at each data point. A vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        Estimated linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the code to compute the closed form solution to linear\n",
    "    regression and put the result in theta.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Look up the function `np.linalg.pinv` for computing matrix inverse.\n",
    "    \"\"\"\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "\n",
    "    \n",
    "    # =================================================================\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5z35UXfPZlp"
   },
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVHzCnSPPZlq"
   },
   "outputs": [],
   "source": [
    "grader[7] = normalEqn\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCSFWPyfPZlt"
   },
   "source": [
    "Optional (ungraded) exercise: Now, once you have found $\\theta$ using this\n",
    "method, use it to make a price prediction for a 1650-square-foot house with\n",
    "3 bedrooms. You should find that gives the same predicted price as the value\n",
    "you obtained using the model fit with gradient descent (in Section 3.2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfnRwLLxPZlv"
   },
   "outputs": [],
   "source": [
    "# Calculate the parameters from the normal equation\n",
    "theta = normalEqn(X, y);\n",
    "\n",
    "# Display normal equation's result\n",
    "print('Theta computed from the normal equations: {:s}'.format(str(theta)));\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "price = 0 # You should change this\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using normal equations): ${:.0f}'.format(price))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wf6YuAf_PZlb"
   ],
   "name": "exercise1_kor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

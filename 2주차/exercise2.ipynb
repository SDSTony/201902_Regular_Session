{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sMM-6O5ykez"
   },
   "source": [
    "# SKKU TNT 2019-2\n",
    "\n",
    "## Programming Exercise 2: Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "░░░░▄▄▄▄▀▀▀▀▀▀▀▀▄▄▄▄▄▄\n",
      "░░░░█░░░░▒▒▒▒▒▒▒▒▒▒▒▒░░▀▀▄\n",
      "░░░█░░░▒▒▒▒▒▒░░░░░░░░▒▒▒░░█\n",
      "░░█░░░░░░▄██▀▄▄░░░░░▄▄▄░░░█\n",
      "░▀▒▄▄▄▒░█▀▀▀▀▄▄█░░░██▄▄█░░░█\n",
      "█▒█▒▄░▀▄▄▄▀░░░░░░░░█░░░▒▒▒▒▒█\n",
      "█▒█░█▀▄▄░░░░░█▀░░░░▀▄░░▄▀▀▀▄▒█ \n",
      "░█▀▄░█▄░█▀▄▄░▀░▀▀░▄▄▀░░░░█░░█\n",
      "░░█░░▀▄▀█▄▄░█▀▀▀▄▄▄▄▀▀█▀██░█\n",
      "░░░█░░██░░▀█▄▄▄█▄▄█▄████░█   헤헤헤... 안타깝지만 다시해보라구...\n",
      "░░░░█░░░▀▀▄░█░░░█░███████░█\n",
      "░░░░░▀▄░░░▀▀▄▄▄█▄█▄█▄█▄▀░░█\n",
      "░░░░░░░▀▄▄░▒▒▒▒░░░░░░░░░░█\n",
      "░░░░░░░░░░▀▀▄▄░▒▒▒▒▒▒▒▒▒▒░█\n",
      "░░░░░░░░░░░░░░▀▄▄▄▄▄░░░░░█\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "░░░░▄▄▄▄▀▀▀▀▀▀▀▀▄▄▄▄▄▄\n",
    "░░░░█░░░░▒▒▒▒▒▒▒▒▒▒▒▒░░▀▀▄\n",
    "░░░█░░░▒▒▒▒▒▒░░░░░░░░▒▒▒░░█\n",
    "░░█░░░░░░▄██▀▄▄░░░░░▄▄▄░░░█\n",
    "░▀▒▄▄▄▒░█▀▀▀▀▄▄█░░░██▄▄█░░░█\n",
    "█▒█▒▄░▀▄▄▄▀░░░░░░░░█░░░▒▒▒▒▒█\n",
    "█▒█░█▀▄▄░░░░░█▀░░░░▀▄░░▄▀▀▀▄▒█ \n",
    "░█▀▄░█▄░█▀▄▄░▀░▀▀░▄▄▀░░░░█░░█\n",
    "░░█░░▀▄▀█▄▄░█▀▀▀▄▄▄▄▀▀█▀██░█\n",
    "░░░█░░██░░▀█▄▄▄█▄▄█▄████░█   헤헤헤... 안타깝지만 다시해보라구...\n",
    "░░░░█░░░▀▀▄░█░░░█░███████░█\n",
    "░░░░░▀▄░░░▀▀▄▄▄█▄█▄█▄█▄▀░░█\n",
    "░░░░░░░▀▄▄░▒▒▒▒░░░░░░░░░░█\n",
    "░░░░░░░░░░▀▀▄▄░▒▒▒▒▒▒▒▒▒▒░█\n",
    "░░░░░░░░░░░░░░▀▄▄▄▄▄░░░░░█\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─────────▄▄───────────────────▄▄──\n",
      "──────────▀█───────────────────▀█─\n",
      "──────────▄█───────────────────▄█─\n",
      "──█████████▀───────────█████████▀─\n",
      "───▄██████▄─────────────▄██████▄──\n",
      "─▄██▀────▀██▄─────────▄██▀────▀██▄\n",
      "─██────────██─────────██────────██\n",
      "─██───██───██─────────██───██───██\n",
      "─██────────██─────────██────────██\n",
      "──██▄────▄██───────────██▄────▄██─\n",
      "───▀██████▀─────────────▀██████▀──\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "───────────█████████████────────── 오우... 좀 하는데?? 다음 단계로 진행하라구!!\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "─────────▄▄───────────────────▄▄──\n",
    "──────────▀█───────────────────▀█─\n",
    "──────────▄█───────────────────▄█─\n",
    "──█████████▀───────────█████████▀─\n",
    "───▄██████▄─────────────▄██████▄──\n",
    "─▄██▀────▀██▄─────────▄██▀────▀██▄\n",
    "─██────────██─────────██────────██\n",
    "─██───██───██─────────██───██───██\n",
    "─██────────██─────────██────────██\n",
    "──██▄────▄██───────────██▄────▄██─\n",
    "───▀██████▀─────────────▀██████▀──\n",
    "──────────────────────────────────\n",
    "──────────────────────────────────\n",
    "──────────────────────────────────\n",
    "───────────█████████████────────── 오우... 좀 하는데?? 다음 단계로 진행하라구!!\n",
    "──────────────────────────────────\n",
    "──────────────────────────────────\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "┈┈╱╲┈┈┈┈╱╲┈┈╭━╮┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╱╲┈┈┈┈╱╲┈┈╭━╮┈                         \n",
      "┈╱╱╲╲__╱╱╲╲┈╰╮┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╱╱╲╲__╱╱╲╲┈╰╮┃┈\n",
      "┈▏┏┳╮┈╭┳┓▕┈┈┃┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈▏┏┳╮┈╭┳┓▕┈┈┃┃┈\n",
      "┈▏╰┻┛▼┗┻╯▕┈┈┃┃┈축하합니다 정답입니다옹┈┈▏╰┻┛▼┗┻╯▕┈┈┃┃┈\n",
      "┈╲┈┈╰┻╯┈┈╱▔▔┈┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╲┈┈╰┻╯┈┈╱▔▔┈┃┈\n",
      "┈┈╰━┳━━━╯┈┈┈┈┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╰━┳━━━╯┈┈┈┈┃┈\n",
      "┈┈┈┈┃┏┓┣━━┳┳┓┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┃┏┓┣━━┳┳┓┃┈\n",
      "┈┈┈┈┗┛┗┛┈┈┗┛┗┛┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┗┛┗┛┈┈┗┛┗┛┈\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "┈┈╱╲┈┈┈┈╱╲┈┈╭━╮┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╱╲┈┈┈┈╱╲┈┈╭━╮┈                         \n",
    "┈╱╱╲╲__╱╱╲╲┈╰╮┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╱╱╲╲__╱╱╲╲┈╰╮┃┈\n",
    "┈▏┏┳╮┈╭┳┓▕┈┈┃┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈▏┏┳╮┈╭┳┓▕┈┈┃┃┈\n",
    "┈▏╰┻┛▼┗┻╯▕┈┈┃┃┈축하합니다 정답입니다옹┈┈▏╰┻┛▼┗┻╯▕┈┈┃┃┈\n",
    "┈╲┈┈╰┻╯┈┈╱▔▔┈┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╲┈┈╰┻╯┈┈╱▔▔┈┃┈\n",
    "┈┈╰━┳━━━╯┈┈┈┈┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈╰━┳━━━╯┈┈┈┈┃┈\n",
    "┈┈┈┈┃┏┓┣━━┳┳┓┃┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┃┏┓┣━━┳┳┓┃┈\n",
    "┈┈┈┈┗┛┗┛┈┈┗┛┗┛┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┗┛┗┛┈┈┗┛┗┛┈\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________________▄▄▄▀▀▀▀▀▀▀▄\n",
      " _______________▄▀▀____▀▀▀▀▄____█\n",
      " ___________▄▀▀__▀▀▀▀▀▀▄___▀▄___█\n",
      " __________█▄▄▄▄▄▄_______▀▄__▀▄__█\n",
      " _________█_________▀▄______█____█_█\n",
      " ______▄█_____________▀▄_____▐___▐_▌\n",
      " ______██_______________▀▄___▐_▄▀▀▀▄\n",
      " ______█________██_______▌__▐▄▀______█\n",
      " ______█_________█_______▌__▐▐________▐\n",
      " _____▐__________▌_____▄▀▀▀__▌_______▐_____________▄▄▄▄▄▄\n",
      " ______▌__________▀▀▀▀________▀▀▄▄▄▀______▄▄████▓▓▓▓▓▓▓███▄\n",
      " ______▌____________________________▄▀__▄▄█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▄\n",
      " ______▐__________________________▄▀_▄█▓▓▓▓▓▓▓▓▓▓_____▓▓____▓▓█▄\n",
      " _______▌______________________▄▀_▄█▓▓▓▓▓▓▓▓▓▓▓____▓▓_▓▓_▓▓__▓▓█\n",
      " _____▄▀▄_________________▄▀▀▌██▓▓▓▓▓▓▓▓▓▓▓▓▓__▓▓▓___▓▓_▓▓__▓▓█\n",
      " ____▌____▀▀▀▄▄▄▄▄▄▄▄▀▀___▌█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓__▓________▓▓___▓▓▓█\n",
      " _____▀▄_________________▄▀▀▓▓▓▓▓▓▓▓█████████████▄▄_____▓▓__▓▓▓█\n",
      " _______█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓▓▓▓▓██▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓██▄▄___▓▓▓▓▓█\n",
      " _______█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓▓███▓▓▓▓████▓▓▓▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓▓█\n",
      " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓█▓▓██░░███████░██▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓█\n",
      " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓██▓░░░░░█░░░░░██░░░░██▓▓▓▓▓▓▓▓▓██▓▓▓▓▌\n",
      " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓███░░░░░░░░____░██░░░░░░░██▓▓▓▓▓▓▓██▓▓▌\n",
      " ________▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░░░________░░░░░░░░░██████▓▓▓▓▓█▓▌\n",
      " ________▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░░___▓▓▓▓▓░░░░░░░███░░███▓▓▓▓▓█▓▌\n",
      " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░___▓▓█▄▄▓░░░░░░░░___░░░░█▓▓▓▓▓█▓▌\n",
      " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░░█░░░___▓▓██░░░░░░░░▓▓▓▓__░░░░█▓▓▓▓██\n",
      " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░███░░____▓░░░░░░░░░░░█▄█▓__░░░░█▓▓█▓█\n",
      " _________▐▓▓▓▓▓▓▓▓▓▓▓▓▓█░█████░░░░░░░░░░░░░░░░░█▓__░░░░███▓█\n",
      " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░███████░░░░░░░░░░░░░░░▓_░░░░░██▓█\n",
      " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░███████░░░░░░░░░░░░░░_░░░░░██▓█\n",
      " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░███████░░░░░░░░░░░░░░░░░░░██▓█\n",
      " ___________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░░███████░░░░░░░░░░░█████░██░░░\n",
      " ___________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░__███████░░░░░███████░░█░░░░\n",
      " ___________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░█▄▄▄▀▀▀▀████████████░░█░░░░\n",
      " ___________▐▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░██████▄__▀▀░░░███░░░░░█░░░\n",
      " ___________▐▓▓▓▓▓▓▓▓▓▓▓█▒█░░░░░░▓▓▓▓▓███▄░░░░░░░░░░░░░░░______▄▄▄\n",
      " ___________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█░░░░░░▓▓▓▓▓█░░░░░░░░░░░░░░░▄▄▄_▄▀▀____▀▄\n",
      " __________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█▓▓░░░░░░░░░░░░░░░░░░░░░____▄▀____▀▄_________▀▄\n",
      " _________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█▓▓▓▓░░░░░░░░░░░░░░░░░______▐▄________█▄▄▀▀▀▄__█\n",
      " ________█▓▓▓▓▓▓▓▓█▒▒▒▒▒▒█▓▓▓▓▓▓▓░░░░░░░░░____________█_█______▐_________▀▄▌\n",
      " _______█▓▓▓▓▓▓▓▓█▒▒▒▒▒▒███▓▓▓▓▓▓▓▓▓▓▓█▒▒▄___________█__▀▄____█____▄▄▄____▐\n",
      " ______█▓▓▓▓▓▓▓█_______▒▒█▒▒██▓▓▓▓▓▓▓▓▓▓█▒▒▒▄_________█____▀▀█▀▄▀▀▀___▀▀▄▄▐\n",
      " _____█▓▓▓▓▓██▒_________▒█▒▒▒▒▒███▓▓▓▓▓▓█▒▒▒██________▐_______▀█_____________█\n",
      " ____█▓▓████▒█▒_________▒█▒▒▒▒▒▒▒▒███████▒▒▒▒██_______█_______▐______▄▄▄_____█\n",
      " __█▒██▒▒▒▒▒▒█▒▒____▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒____▒█▓█__▄█__█______▀▄▄▀▀____▀▀▄▄█\n",
      " __█▒▒▒▒▒▒▒▒▒▒█▒▒▒████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▓▓█▓▓▌_▐________▐____________▐\n",
      " __█▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒_______█▓▓▓█▓▌__▌_______▐_____▄▄____▐\n",
      " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒_____█▓▓▓█▓▓▌__▌_______▀▄▄▀______▐\n",
      " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███████▓▓█▓▓▓▌__▀▄_______________▄▀\n",
      " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒██▓▓▓▓▓▌___▀▄_________▄▀▀\n",
      " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▒▒▒█▓▓▓▓▓▀▄__▀▄▄█▀▀▀\n",
      " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▓▓▓▓██▄▄▄▀\n",
      " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████\n",
      " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▄▄▄▄▄\n",
      " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒██▄▄\n",
      " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒▒▒▒▒▒▒█▄\n",
      " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " ___█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░▒▒▒▒▒▒▌\n",
      " ____█▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒░░░░░░░░░░░░░▒▒▌\n",
      " ____█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█████████████▒▒▒▒▒█▒▒▒▒▒▒▒▒░░░░▒▒▒▒▒▒▒▒▒▒▒░▒▌\n",
      " _____█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______▐▒▒▒▒█▒▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
      " ______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒█▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▌\n",
      " _______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒█▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▌\n",
      " ________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒▒▒▒░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒▒░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒░░░░░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
      " __________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒░░░▒▒▒▒▒░░░░░░░░▒▒▒█▀▀▀\n",
      " ___________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█░▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░█▀\n",
      " ____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
      " _____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
      " _____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______▀▀▀███████▀▀\n",
      " ______________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _______________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " ________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " __________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒█\n",
      " ___________________█▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒█\n",
      " ___________________█▒▒▒▒▒▒▒▒████▒▒▒▒▒▒▒█\n",
      " ___________________█████████▒▒▒▒▒▒▒▒▒▒▒█\n",
      " ____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " ____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
      " _____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
      " _____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
      " ______________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░▌\n",
      " _______________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░█\n",
      " ________________________█▒▒▒▒▒▒▒▒▒▒▒░░░█\n",
      " __________________________██▒▒▒▒▒▒░░░█▀\n",
      " _____________________________█░░░░░█▀\n",
      " _______________________________▀▀▀▀\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "___________________▄▄▄▀▀▀▀▀▀▀▄\n",
    " _______________▄▀▀____▀▀▀▀▄____█\n",
    " ___________▄▀▀__▀▀▀▀▀▀▄___▀▄___█\n",
    " __________█▄▄▄▄▄▄_______▀▄__▀▄__█\n",
    " _________█_________▀▄______█____█_█\n",
    " ______▄█_____________▀▄_____▐___▐_▌\n",
    " ______██_______________▀▄___▐_▄▀▀▀▄\n",
    " ______█________██_______▌__▐▄▀______█\n",
    " ______█_________█_______▌__▐▐________▐\n",
    " _____▐__________▌_____▄▀▀▀__▌_______▐_____________▄▄▄▄▄▄\n",
    " ______▌__________▀▀▀▀________▀▀▄▄▄▀______▄▄████▓▓▓▓▓▓▓███▄\n",
    " ______▌____________________________▄▀__▄▄█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▄\n",
    " ______▐__________________________▄▀_▄█▓▓▓▓▓▓▓▓▓▓_____▓▓____▓▓█▄\n",
    " _______▌______________________▄▀_▄█▓▓▓▓▓▓▓▓▓▓▓____▓▓_▓▓_▓▓__▓▓█\n",
    " _____▄▀▄_________________▄▀▀▌██▓▓▓▓▓▓▓▓▓▓▓▓▓__▓▓▓___▓▓_▓▓__▓▓█\n",
    " ____▌____▀▀▀▄▄▄▄▄▄▄▄▀▀___▌█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓__▓________▓▓___▓▓▓█\n",
    " _____▀▄_________________▄▀▀▓▓▓▓▓▓▓▓█████████████▄▄_____▓▓__▓▓▓█\n",
    " _______█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓▓▓▓▓██▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓██▄▄___▓▓▓▓▓█\n",
    " _______█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓▓███▓▓▓▓████▓▓▓▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓▓█\n",
    " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓█▓█▓▓██░░███████░██▓▓▓▓▓▓▓▓▓▓██▓▓▓▓▓█\n",
    " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓██▓░░░░░█░░░░░██░░░░██▓▓▓▓▓▓▓▓▓██▓▓▓▓▌\n",
    " ________█▓▓▓▓▓▓▓▓▓▓▓▓▓▓███░░░░░░░░____░██░░░░░░░██▓▓▓▓▓▓▓██▓▓▌\n",
    " ________▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░░░________░░░░░░░░░██████▓▓▓▓▓█▓▌\n",
    " ________▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░░___▓▓▓▓▓░░░░░░░███░░███▓▓▓▓▓█▓▌\n",
    " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓██░░░░░___▓▓█▄▄▓░░░░░░░░___░░░░█▓▓▓▓▓█▓▌\n",
    " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░░█░░░___▓▓██░░░░░░░░▓▓▓▓__░░░░█▓▓▓▓██\n",
    " _________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░███░░____▓░░░░░░░░░░░█▄█▓__░░░░█▓▓█▓█\n",
    " _________▐▓▓▓▓▓▓▓▓▓▓▓▓▓█░█████░░░░░░░░░░░░░░░░░█▓__░░░░███▓█\n",
    " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░███████░░░░░░░░░░░░░░░▓_░░░░░██▓█\n",
    " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░███████░░░░░░░░░░░░░░_░░░░░██▓█\n",
    " __________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░███████░░░░░░░░░░░░░░░░░░░██▓█\n",
    " ___________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░░███████░░░░░░░░░░░█████░██░░░\n",
    " ___________█▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░__███████░░░░░███████░░█░░░░\n",
    " ___________█▓▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░█▄▄▄▀▀▀▀████████████░░█░░░░\n",
    " ___________▐▓▓▓▓▓▓▓▓▓▓▓▓█░░░░░░██████▄__▀▀░░░███░░░░░█░░░\n",
    " ___________▐▓▓▓▓▓▓▓▓▓▓▓█▒█░░░░░░▓▓▓▓▓███▄░░░░░░░░░░░░░░░______▄▄▄\n",
    " ___________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█░░░░░░▓▓▓▓▓█░░░░░░░░░░░░░░░▄▄▄_▄▀▀____▀▄\n",
    " __________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█▓▓░░░░░░░░░░░░░░░░░░░░░____▄▀____▀▄_________▀▄\n",
    " _________█▓▓▓▓▓▓▓▓▓█▒▒▒▒█▓▓▓▓░░░░░░░░░░░░░░░░░______▐▄________█▄▄▀▀▀▄__█\n",
    " ________█▓▓▓▓▓▓▓▓█▒▒▒▒▒▒█▓▓▓▓▓▓▓░░░░░░░░░____________█_█______▐_________▀▄▌\n",
    " _______█▓▓▓▓▓▓▓▓█▒▒▒▒▒▒███▓▓▓▓▓▓▓▓▓▓▓█▒▒▄___________█__▀▄____█____▄▄▄____▐\n",
    " ______█▓▓▓▓▓▓▓█_______▒▒█▒▒██▓▓▓▓▓▓▓▓▓▓█▒▒▒▄_________█____▀▀█▀▄▀▀▀___▀▀▄▄▐\n",
    " _____█▓▓▓▓▓██▒_________▒█▒▒▒▒▒███▓▓▓▓▓▓█▒▒▒██________▐_______▀█_____________█\n",
    " ____█▓▓████▒█▒_________▒█▒▒▒▒▒▒▒▒███████▒▒▒▒██_______█_______▐______▄▄▄_____█\n",
    " __█▒██▒▒▒▒▒▒█▒▒____▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒____▒█▓█__▄█__█______▀▄▄▀▀____▀▀▄▄█\n",
    " __█▒▒▒▒▒▒▒▒▒▒█▒▒▒████▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▓▓█▓▓▌_▐________▐____________▐\n",
    " __█▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒_______█▓▓▓█▓▌__▌_______▐_____▄▄____▐\n",
    " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒_____█▓▓▓█▓▓▌__▌_______▀▄▄▀______▐\n",
    " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███████▓▓█▓▓▓▌__▀▄_______________▄▀\n",
    " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒██▓▓▓▓▓▌___▀▄_________▄▀▀\n",
    " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▒▒▒█▓▓▓▓▓▀▄__▀▄▄█▀▀▀\n",
    " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▓▓▓▓██▄▄▄▀\n",
    " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒████\n",
    " █▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▄▄▄▄▄\n",
    " _█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒██▄▄\n",
    " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒███▒▒▒▒▒▒▒▒▒▒▒▒▒█▄\n",
    " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " __█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " ___█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░▒▒▒▒▒▒▌\n",
    " ____█▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒▒▒▒█▒▒▒▒█▒▒▒▒▒▒█▒▒▒▒▒▒▒▒▒░░░░░░░░░░░░░▒▒▌\n",
    " ____█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█████████████▒▒▒▒▒█▒▒▒▒▒▒▒▒░░░░▒▒▒▒▒▒▒▒▒▒▒░▒▌\n",
    " _____█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______▐▒▒▒▒█▒▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
    " ______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒█▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▌\n",
    " _______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒█▒▒▒▒▒▒░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▌\n",
    " ________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒▒▒▒░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒▒░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█________█▒▒▒░░░░░░░▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
    " __________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒░░░▒▒▒▒▒░░░░░░░░▒▒▒█▀▀▀\n",
    " ___________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█░▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░█▀\n",
    " ____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
    " _____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______█▒▒▒▒▒▒▒▒▒▒▒▒█▀\n",
    " _____________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█_______▀▀▀███████▀▀\n",
    " ______________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _______________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " ________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " __________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒██▒█\n",
    " ___________________█▒▒▒▒▒▒▒▒▒▒▒▒▒██▒▒▒▒█\n",
    " ___________________█▒▒▒▒▒▒▒▒████▒▒▒▒▒▒▒█\n",
    " ___________________█████████▒▒▒▒▒▒▒▒▒▒▒█\n",
    " ____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " ____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█\n",
    " _____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
    " _____________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░▌\n",
    " ______________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░▌\n",
    " _______________________█▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░█\n",
    " ________________________█▒▒▒▒▒▒▒▒▒▒▒░░░█\n",
    " __________________________██▒▒▒▒▒▒░░░█▀\n",
    " _____________________________█░░░░░█▀\n",
    " _______________________________▀▀▀▀\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSmdftRxyke1"
   },
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정에 사용되는 패키지\n",
    "import os\n",
    "\n",
    "# 벡터 계산\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 시각화 라이브러비\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "# 과제 제출을 위한 라이브러리\n",
    "import utils \n",
    "\n",
    "# 이 연습에 대한 제출, 채점 라이브러리\n",
    "grader = utils.Grader()\n",
    "\n",
    "# matplotlib가 노트북에 플롯을 포함하도록 지시\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3Qhl3hMyke7"
   },
   "source": [
    "지난주 자료를 성공적으로 완성했다면 TNT 코드1팀이 앤드류 응 교수님 대신 칭찬해 드리도록 하죠! 이제 당신은 선형 회귀를 이해했고 자신의 데이터 셋에서 선형 회귀를 사용할 수 있습니다.\n",
    "\n",
    "이 프로그래밍 연습의 나머지 부분에서는 다음과 같은 선택적 연습이 포함되었습니다. 이 연습은 자료에 대한 심층적인 이해를 돕기 때문에 가능하면 이를 해보는 것이 좋습니다. 이 연습에 대한 답안을 제출하고 제촐한 답안이 올바른지 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "## 3 Linear regression with multiple variables\n",
    "\n",
    "이 부분에서는 주택의 가격을 예측하기 위해 여러 변수로 선형 회귀를 구현합니다. 집을 팔고 있고 좋은 시장 가격이 얼마인지 알고 싶다고 가정 해 봅시다. 이를 수행하는 한 가지 방법은 먼저 판매 된 최근 주택에 대한 정보를 수집하고 주택 가격 모델을 만드는 것입니다.\n",
    "\n",
    "`Data / ex1data2.txt` 파일에는 오리건 주 포틀랜드에있는 주택 가격에 대한 훈련 세트가 포함되어 있습니다. 첫 번째 열은 집의 크기 (n square feet), 두 번째 열은 침실 수, 세 번째 열은 가격입니다.\n",
    "\n",
    "<a id=\"section4\"></a>\n",
    "\n",
    "### 3.1 Feature Normalization\n",
    "\n",
    "이 데이터 세트에서 일부 값을로드하고 표시하는 것으로 시작합시다. 값을 살펴보면 집 크기는 침실 수의 약 1000 배입니다. Feature가 차수에 따라 다르면, 처음 수행하는 Feature scaling은 gradient descent를 훨씬 더 빠르게 수렴시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpHmyjNjyke8",
    "outputId": "422733d8-d0c9-4e98-8edf-63148f9b37c6"
   },
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>399900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>329900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>369000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>539900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>314900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1427.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>198999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1380.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>212000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>242500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1   X2         y\n",
       "0  2104.0  3.0  399900.0\n",
       "1  1600.0  3.0  329900.0\n",
       "2  2400.0  3.0  369000.0\n",
       "3  1416.0  2.0  232000.0\n",
       "4  3000.0  4.0  539900.0\n",
       "5  1985.0  4.0  299900.0\n",
       "6  1534.0  3.0  314900.0\n",
       "7  1427.0  3.0  198999.0\n",
       "8  1380.0  3.0  212000.0\n",
       "9  1494.0  3.0  242500.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "source": [
    "# data 불러오기\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "m = y.size\n",
    "\n",
    "pd.DataFrame({'X1':X[:,0], 'X2':X[:,1], 'y':y}).iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avY9yP96ykfC"
   },
   "source": [
    "`featureNormalize` 함수에서 코드를 완성해봅시다.\n",
    "\n",
    "* dataset에서 각 feature의 평균값을 뺍니다.\n",
    "* 평균을 뺀 후 feature를 각각의 표준 편차로 스케일링(나누기)합니다.\n",
    "\n",
    "표준 편차는 특정 피쳐의 값 범위에 얼마나 많은 변화가 있는지를 측정하는 방법입니다 (대부분의 데이터 포인트는 평균의 ± 2 표준 편차 내에 있음). 이것은 값의 범위 (max-min)를 대신하는 대안입니다. `numpy`에서`std` 함수를 사용하여 표준 편차를 계산할 수 있습니다.\n",
    "\n",
    "예를 들어, $X [:, 0]$ 에는 트레이닝 세트 $ x_1 $ (집 크기)의 모든 값이 포함되므로 `np.std (X [:, 0])`는 집크기 feature의 표준 편차를 계산합니다.\n",
    "\n",
    "함수 featureNormalize가 호출 될 때 $ x_0 = 1 $에 해당하는 1의 추가 열이 아직 $ X $에 추가되지 않았습니다.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:**\n",
    "기능을 정규화 할 때는 정규화에 사용 된 값 (평균값 및 계산에 사용 된 표준 편차)을 저장하는 것이 중요합니다. 모델에서 매개 변수를 학습 한 후 종종 우리가 전에 보지 못한 주택의 가격을 예측하려고 합니다. 새로운 x 값 (거실 공간 및 침실 수)이 주어지면 먼저 훈련 세트에서 이전에 계산 한 평균 및 표준 편차를 사용하여 x를 정규화해야합니다.\n",
    "</div>\n",
    "<a id=\"featureNormalize\"></a>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 및 표준편차 구하기\n",
    "# 직접 작성해주세요.\n",
    "mu = X.mean(axis=0) # X의 평균\n",
    "std = X.std(axis=0) # X의 표준편차\n",
    "\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "print(std)\n",
    "print(std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pX0OhUbHykfD"
   },
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    \"\"\"\n",
    "    X의 피처를 정규화합니다.\n",
    "    각 피처의 평균값이 0이고 표준 편차가 1 인 정규화 된 X 버전을 반환합니다.\n",
    "    이는 종종 학습 알고리즘으로 작업 할 때 수행하는, 괜찮은 전처리 단계입니다.\n",
    "    \n",
    "    [Parameters]\n",
    "    ----------\n",
    "    X : array 형태 (m x n).\n",
    "    \n",
    "    [Returns]\n",
    "    -------\n",
    "    X_norm : 정규화된 data set의 array 형태 (m x n)\n",
    "    \n",
    "    [Instructions]\n",
    "    ------------\n",
    "    먼저 각 피처 차원에 대해 피처의 평균을 계산하고\n",
    "    데이터 집합에서 빼고 평균 값을 mu로 저장합니다.\n",
    "    그런 다음 각 피처의 표준 편차를 계산하고\n",
    "    각 피처를 표준 편차로 나누고 표준 편차를 시그마로 저장합니다.\n",
    "    \n",
    "    X는 각 열이 피처이고 각 행이 예인 행렬입니다.\n",
    "    각 기능에 대해 표준화를 별도로 수행해야합니다.\n",
    "    \n",
    "    [Hint]\n",
    "    ----\n",
    "    'np.mean'및 'np.std'기능 사용을 권장합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 아래 값을 올바르게 설정해야합니다\n",
    "    # =========================== YOUR CODE HERE =====================\n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    # ================================================================\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwVwK2_8ykfG"
   },
   "source": [
    "구현 된`featureNormalize` 기능을 실행하려면 다음 셀을 실행하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_9VmwAUykfH",
    "outputId": "f9b4c6d7-72ef-40db-fed0-390772e57c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean: [2000.68085106    3.17021277]\n",
      "Computed standard deviation: [7.86202619e+02 7.52842809e-01]\n"
     ]
    }
   ],
   "source": [
    "# 로드 된 데이터에서 \"featureNormalize\"호출\n",
    "X_norm, mu, sigma = featureNormalize(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScTej6NCykfK"
   },
   "source": [
    "*정답을 제출하세요*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32xQcyPgykfL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─────────▄▄───────────────────▄▄──\n",
      "──────────▀█───────────────────▀█─\n",
      "──────────▄█───────────────────▄█─\n",
      "──█████████▀───────────█████████▀─\n",
      "───▄██████▄─────────────▄██████▄──\n",
      "─▄██▀────▀██▄─────────▄██▀────▀██▄\n",
      "─██────────██─────────██────────██\n",
      "─██───██───██─────────██───██───██\n",
      "─██────────██─────────██────────██\n",
      "──██▄────▄██───────────██▄────▄██─\n",
      "───▀██████▀─────────────▀██████▀──\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "───────────█████████████────────── 오우... 좀 하는데?? 다음 단계로 진행하라구!!\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader.answer[0] = featureNormalize\n",
    "grader.grade(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuB4PWCIykfN"
   },
   "source": [
    "`featureNormalize` 함수가 테스트 된 후, 이제 우리는 절편 항(상수항)을`X_norm`에 추가합니다 :"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bCgY64IykfO"
   },
<<<<<<< HEAD
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.13141542, -0.22609337],\n",
       "       [ 1.        , -0.5096407 , -0.22609337],\n",
       "       [ 1.        ,  0.5079087 , -0.22609337],\n",
       "       [ 1.        , -0.74367706, -1.5543919 ],\n",
       "       [ 1.        ,  1.27107075,  1.10220517],\n",
       "       [ 1.        , -0.01994505,  1.10220517],\n",
       "       [ 1.        , -0.59358852, -0.22609337],\n",
       "       [ 1.        , -0.72968575, -0.22609337],\n",
       "       [ 1.        , -0.78946678, -0.22609337],\n",
       "       [ 1.        , -0.64446599, -0.22609337]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> 65ee30cbe28da9eb9e72deacc57ed92bbf4de82f
   "source": [
    "# X에 절편 항 추가하기\n",
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pV2DC_p9ykfR"
   },
   "source": [
    "<a id=\"section5\"></a>\n",
    "### Gradient Descent (경사하강)\n",
    "\n",
    "이전에는 일 변량 회귀 문제에 대해 경사 하강을 구현했습니다. 유일한 차이점은 $ X $ 매트릭스에 하나 이상의 기능이 있다는 것입니다. 가설 함수 및 배치 경사 하강에 대한업데이트 규칙은 변경되지 않습니다.\n",
    "\n",
    "여러 변수를 가진 선형 회귀에 대한 비용 함수와 경사 하강을 구현하려면 'computeCostMulti'및 'gradientDescentMulti'함수의 코드를 완성해야합니다. 이전 부분의 코드 (단일 변수)가 이미 여러 변수를 지원하는 경우 여기에서도 사용할 수 있습니다.\n",
    "\n",
    "코드가 여러 기능을 지원하는지, 그리고 정상적으로 벡터화되어 있는지 확인하십시오.\n",
    "`numpy` 배열의`shape` 속성을 사용하여 데이터 셋에 몇 개의 기능이 있는지 알아낼 수 있습니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** 다변량 경우 비용 함수는 다음과 같은 벡터화 된 형태로 작성 될 수 있습니다.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
    "\n",
    "다음과 같을 때!\n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "          - (x^{(1)})^T - \\\\\n",
    "          - (x^{(2)})^T - \\\\\n",
    "          \\vdots \\\\\n",
    "          - (x^{(m)})^T - \\\\ \\\\\n",
    "        \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n",
    "\n",
    "벡터화 된 버전은 'numpy'와 같은 수치 계산 도구로 작업 하게 된다면 효율적으로 수행할 수 있습니다. 당신이 행렬 연산을 쫌 한다면 두 형식이 동일하다는 것을 알 수 있을 것입니다.\n",
    "</div>\n",
    "\n",
    "<a id=\"computeCostMulti\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48x5qKLNykfS"
   },
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    \"\"\"\n",
    "    여러 변수를 사용한 선형 회귀 분석 비용 함수를 계산하세요.\n",
    "    데이터 포인트 i에 맞게 선형 회귀 매개 변수로 theta를 사용하는 비용을 계산합니다\n",
    "    \n",
    "    [Parameters]\n",
    "    ----------\n",
    "    X : array 형태 (m x n+1).\n",
    "    \n",
    "    y : array 형태 (m, )\n",
    "    \n",
    "    theta : array 형태 (n+1, ). 선형 회귀 parameters.\n",
    "    \n",
    "    \n",
    "    [Returns]\n",
    "    -------\n",
    "    J : float, 비용 함수의 값\n",
    "    \n",
    "    [Instructions]\n",
    "    ------------\n",
    "    선택한 세타의 비용을 계산합니다. J를 비용으로 설정해야합니다.\n",
    "    \"\"\"\n",
    "    # 사용할 values를 초기화 합시다\n",
    "    m = y.shape[0] # training examples의 개수\n",
    "    \n",
    "    # 다음 변수를 올바르게 반환해야합니다.\n",
    "    # ======================= YOUR CODE HERE ===========================\n",
    "    \n",
    "    hypothesis = np.dot(X, theta)\n",
    "    J = np.dot((hypothesis - y), (hypothesis - y)) / (2*m)\n",
    "    \n",
    "    # ==================================================================\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qR7lN4nMykfV"
   },
   "source": [
    "*정답을 제출하세요*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R32yQ4MykfW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─────────▄▄───────────────────▄▄──\n",
      "──────────▀█───────────────────▀█─\n",
      "──────────▄█───────────────────▄█─\n",
      "──█████████▀───────────█████████▀─\n",
      "───▄██████▄─────────────▄██████▄──\n",
      "─▄██▀────▀██▄─────────▄██▀────▀██▄\n",
      "─██────────██─────────██────────██\n",
      "─██───██───██─────────██───██───██\n",
      "─██────────██─────────██────────██\n",
      "──██▄────▄██───────────██▄────▄██─\n",
      "───▀██████▀─────────────▀██████▀──\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "───────────█████████████────────── 오우... 좀 하는데?? 다음 단계로 진행하라구!!\n",
      "──────────────────────────────────\n",
      "──────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader.answer[1] = computeCostMulti\n",
    "grader.grade(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aG33Wi8Uykfb"
   },
   "source": [
    "<a id=\"gradientDescentMulti\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)} \\\\\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\\\\n",
    "\\theta_2 := \\theta_2 - \\alpha \\frac{1}{m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jfzvTeBykfd"
   },
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    세타를 학습시키기 위해 경사 하강을 수행합니다.\n",
    "    learning rate인 alpha로 num_iters 그레디언트 단계를 수행하여 theta를 업데이트합니다.\n",
    "        \n",
    "    [Parameters]\n",
    "    ----------\n",
    "    X : array 형식 data set (m x n+1).\n",
    "    \n",
    "    y : array 형식. 주어진 데이터 포인트의 값에 대한 모양 (m,)의 벡터\n",
    "    \n",
    "    theta : array 형식의 선형 회귀에 대한 변수. (n+1, )\n",
    "    \n",
    "    alpha : float 형식. 경사하강에서의 learning rate\n",
    "    \n",
    "    num_iters : int 형식. 경사하강에서 시행 횟수\n",
    "\n",
    "    \n",
    "    [Returns]\n",
    "    -------\n",
    "    theta : array 형식 (n+1, ) 선형 회귀로부터 학습된 변수\n",
    "     \n",
    "    J_history : list 형식. 매 시행 횟수마다의 비용함수의 값이 있음\n",
    "\n",
    "    \n",
    "    [Instructions]\n",
    "    ------------\n",
    "    모수 벡터 theta에서 단일 그래디언트 단계를 수행하세요.\n",
    "    디버깅하는 동안 여기에서 비용 함수 (계산 비용) 및 그라디언트의 값을 인쇄해 봐도 좋을 것 같아요.\n",
    "    \"\"\"\n",
    "    # 유용한 value 초기화\n",
    "    m = y.shape[0] # training examples 갯수\n",
    "    \n",
    "    # 경사 하강에 쓰일 theta를 복사해두기\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # ======================= YOUR CODE HERE ==========================\n",
    "        \n",
    "        hypothesis = np.dot(X, theta)\n",
    "        \n",
    "        for j in range(len(theta)):\n",
    "            \n",
    "            theta[j] -= alpha * sum((hypothesis - y) * X[:, j]) / m\n",
    "        \n",
    "        # =================================================================\n",
    "        \n",
    "        # 매 시행 횟수마다 J를 저장\n",
    "        J_history.append(computeCostMulti(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghSODlhjykfg"
   },
   "source": [
    "*정답을 제출하세요*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1VgOPqDykfh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답\n"
     ]
    }
   ],
   "source": [
    "grader.answer[2] = gradientDescentMulti\n",
    "grader.grade(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkYfjFcnykfk"
   },
   "source": [
    "#### Learning rates 선택하기\n",
    "\n",
    "이 연습에서는 데이터 집합에 대해 다른 학습 속도를 시도하고 빠르게 수렴하는 학습 속도를 찾습니다. 다음 코드를 수정하고 학습 속도를 설정하는 코드 부분을 변경하여 학습 속도를 변경할 수 있습니다.\n",
    "\n",
    "`gradientDescentMulti` 함수 구현을 사용하고 선택한 학습 속도로 약 50 회 반복 그라디언트 디센트를 실행하세요. 이 함수는 벡터 $ J $에 $ J (\\ theta) $ 값의 이력을 반환해야합니다.\n",
    "\n",
    "마지막 반복 후 반복 횟수에 대해 J 값을 플로팅합니다.\n",
    "\n",
    "학습 범위 내에서 학습률을 선택한 경우 플롯 그림은 다음 그림과 유사합니다.\n",
    "\n",
    "![](Figures/learning_rate.png)\n",
    "\n",
    "그래프가 매우 다르게 보이면, 특히 $ J (\\ theta) $ 값이 증가하거나 폭증하는 경우 학습 속도를 조정하고 다시 시도하십시오. 학습률 $ \\ alpha $의 값을 이전 값의 약 3 배 (즉, 0.3, 0.1, 0.03, 0.01 등)의 곱셈 단계에서 로그 스케일로 시도하는 것을 추천드립니다. 곡선에서 전체 추세를 확인하고 나서 실행중인 반복 횟수를 조정할 수도 있습니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** 학습률이 너무 높으면 $ J (\\ theta) $가 발산되어 너무 큰 값을 초래할 수 있습니다. 이 경우 numpy에서 NaN이 출력되는 경우가 발생합니다. NaN은 숫자가 아님을 나타내며 종종 -∞ 및 + ∞을 포함하는 정의되지 않은 연산으로 인해 발생합니다.\n",
    "</div>\n",
    "\n",
    "\n",
    "학습률이 변함에 따라 수렴 곡선의 변화에 주목하세요. 학습률이 낮으면 GD가 최적의 값으로 수렴하는 데 시간이 오래 걸립니다. 반대로 학습률이 높으면 경사 하강이 수렴하지 않거나 발산 될 수도 있습니다. 찾은 최고의 학습률을 사용하여 스크립트를 실행하세요. 수렴 할 때까지 기울기 하강을 실행하여 $ \\ theta $의 최종 값을 찾습니다. 다음에는 이 $ \\ theta $ 값을 사용하여 1650 제곱 피트, 침실 3 개인 주택의 값을 예상해보세요. 나중에 값을 사용하여 정규 방정식의 구현을 확인합니다. 이 예측을 할 때 함수를 정규화하는 것을 잊지 마세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD0ycbDCykfm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed from gradient descent: [340412.65957447 109447.79558639  -6578.3539709 ]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[Instructions]\n",
    "------------\n",
    "특정 학습 속도 (alpha)로 GD를 실행하는 코드의 시작을 제공해 드렸습니다.\n",
    "이제 당신의 임무는 먼저 함수-computeCost와 gradientDescent가\n",
    "이 시작 코드와 함께 작동하는지 확인하고, 다변량 회귀를 확인하는 것입니다.\n",
    "그 후, 다른 alpha 값으로 GD를 실행하고 어느 것이 가장 좋은 결과를 가져오는지 확인하세요.\n",
    "마지막으로 코드를 완성하여 1650 평방 피트, 3br 주택의 가격을 예측해보세요!\n",
    "\n",
    "[Hint]\n",
    "----\n",
    "예측할 때 feature 정규화를 적용해야 합니다!\n",
    "\"\"\"\n",
    "# 이 부분을 바꿔서 알파 값을 선택하세요\n",
    "alpha = 0.1\n",
    "num_iters = 400\n",
    "\n",
    "# theta를 초기화하고 GD 실행\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "# 수렴 그래프 그리기\n",
    "pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "pyplot.xlabel('Number of iterations')\n",
    "pyplot.ylabel('Cost J')\n",
    "\n",
    "# GD 결과 표시\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "\n",
    "#  1650 평방 피트, 3br 주택의 가격을 예측해보세요\n",
    "# ======================= YOUR CODE HERE ===========================\n",
    "# X의 첫 번째 열은 모두 1이라는 것을 기억하세요!\n",
    "# 따라서 정규화 할 필요가 없습니다.\n",
    "\n",
    "price = np.dot(theta, np.array([1, (1650 - mu[0]) / sigma[0], (3 - mu[1]) / sigma[1]]))\n",
    "   # 이 부분을 바꾸세요\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sU_lyNTZykfp"
   },
   "source": [
    "이 부분에 대한 답안을 제출하지 않아도됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pek8pM69ykfr"
   },
   "source": [
    "<a id=\"section7\"></a>\n",
    "### Normal Equations(정규방정식)\n",
    "\n",
    "앤드류 응 형님의 강의에서 선형 회귀에 대한 수학적 표현식은 다음과 같습니다.\n",
    "\n",
    "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
    "\n",
    "이 공식을 사용하면 피쳐 스케일링이 필요하지 않으며 한 번의 계산으로 정확한 솔루션을 얻을 수 있습니다. 그래디언트 디센트와 같이 수렴까지 루프가 없습니다. 먼저 변수를 수정하지 않도록 데이터를 다시 로드합니다. feature을 확장 할 필요는 없지만 절편 ($ \\theta_0 $)를 사용하려면 $ X $ 행렬에 1의 열을 추가해야합니다. 다음 셀의 코드는 1의 열을 X에 추가합니다.\n",
    "\n",
    "`np.linalg.pinv (또는 inv)` 를 통해 역행렬을 구할 수 있습니다. pinv는 입력된 행렬의 역행렬이 없을 경우 의사역행렬(pseudo inverse matrix)를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtOBEO6rykfs"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "m = y.size\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DLzFpccykfu"
   },
   "source": [
    "위의 공식을 사용하여 $ \\ theta $를 계산하려면 아래 `normalEqn` 함수의 코드를 완성하십시오.\n",
    "<a id=\"normalEqn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngh45hsKykfv"
   },
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    \"\"\"\n",
    "    정규 방정식을 사용하여 수학적 표현식을 선형 회귀로 계산합니다.\n",
    "    \n",
    "    [Parameters]\n",
    "    ----------\n",
    "    X : array 형태 (m x n+1).\n",
    "    \n",
    "    y : array 형태 (m, ).\n",
    "    \n",
    "    [Returns]\n",
    "    -------\n",
    "    theta : array 형태(n+1, ) 선형 회귀의 계수\n",
    "    \n",
    "    [Instructions]\n",
    "    ------------\n",
    "    선형 회귀에 대한 수학적 표현식을 계산하고, 그 결과를 세타에 넣으려는 코드를 완성하십시오.\n",
    "    \n",
    "    [Hint]\n",
    "    ----\n",
    "    역행렬을 계산하기 위해 함수 'np.linalg.pinv'를 사용해보세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    \n",
    "    inverse = np.linalg.pinv(np.dot(X.T, X))\n",
    "    theta = np.dot(np.dot(inverse, X.T), y)\n",
    "    \n",
    "    # =================================================================\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uhUUPEvykfz"
   },
   "source": [
    "*정답을 제출하세요*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FJwFJmiykf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답\n"
     ]
    }
   ],
   "source": [
    "grader.answer[3] = normalEqn\n",
    "grader.grade(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhGUeOiHykf5"
   },
   "source": [
    "이제 이 방법을 사용하여 $ \\ theta $를 찾은 후 3 개의 침실이있는 1650 평방 피트의 주택 가격을 예측하는 데 사용하십시오. GD에 맞는 모델을 사용하여 얻은 값과 동일한 예측 가격을 제공하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePBnERJDykf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed from the normal equations: [89597.90954435   139.21067402 -8738.01911278]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using normal equations): $293081\n"
     ]
    }
   ],
   "source": [
    "# 정규 방정식에서 parameter를 계산\n",
    "theta = normalEqn(X, y);\n",
    "\n",
    "# 정규 방정식 결과 표시\n",
    "print('Theta computed from the normal equations: {:s}'.format(str(theta)));\n",
    "\n",
    "# 1650 sq-ft, 3 br 주택의 가격을 추정하기\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "price = np.dot(theta, [1, 1650, 3])\n",
    " # 여기를 바꿔보세요\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using normal equations): ${:.0f}'.format(price))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "exercise2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
